{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invariant Risk Minimization\n",
    "\n",
    "Exploration of the paper https://arxiv.org/pdf/1907.02893v2.pdf\n",
    "\n",
    "Pytorch code: https://github.com/facebookresearch/InvariantRiskMinimization/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â 0) Dataset exploration/setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Multiply\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv3D, MaxPool3D\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "# convert to RGB\n",
    "x_train = np.stack((x_train,)*3, axis=-1)\n",
    "x_test = np.stack((x_test,)*3, axis=-1)\n",
    "\n",
    "# normalize\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# binary label\n",
    "y_train = (y_train < 5).astype(int)\n",
    "y_test = (y_test < 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 3)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAOVElEQVR4nO3df4xV9ZnH8c8jBYkDGlh0JNasbCXRilEIanHNhk1DZTERMLGWyIZl1WlMDcXUn93Ejm6MP7J0Y/yjyTRiadPSNAGRNMaiSJZdNQ34YxWFdkYzBnBgQtSUqoEVnv1jzuxOdc73jvecc8+F5/1KJvfe89xzz5OrH86553vv+Zq7C8DJ75S6GwDQGoQdCIKwA0EQdiAIwg4E8ZVWbszMOPUPVMzdbbTlhfbsZrbQzP5gZn1mdk+R1wJQLWt2nN3Mxkn6o6QFkvZJ2iFpmbu/nViHPTtQsSr27JdL6nP3d939qKRfS1pc4PUAVKhI2M+RtHfE433Zsr9gZl1mttPMdhbYFoCCKj9B5+49knokDuOBOhXZs++XdO6Ix1/NlgFoQ0XCvkPSTDObYWYTJH1H0uZy2gJQtqYP4939MzO7TdLvJI2TtNbd3yqtMwClanroramN8ZkdqFwlX6oBcOIg7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IIimp2wGJGny5MnJ+qRJk3Jr11xzTXLds846K1lfs2ZNsn7kyJFkPZpCYTezfkmHJR2T9Jm7zy2jKQDlK2PP/vfufqiE1wFQIT6zA0EUDbtL2mJmr5hZ12hPMLMuM9tpZjsLbgtAAUUP469y9/1mdpak58xsj7tvH/kEd++R1CNJZuYFtwegSYX27O6+P7sdlPSUpMvLaApA+ZoOu5l1mNnk4fuSviVpV1mNAShXkcP4TklPmdnw6/zK3Z8tpSu0zIwZM5L1u+66K1mfN29esj5r1qwv3dNYnX322cn6qlWrKtv2iajpsLv7u5IuKbEXABVi6A0IgrADQRB2IAjCDgRB2IEgzL11X2rjG3TVuOCCC3Jrq1evTq67fPnyZH3ixInJejb0mmvv3r25tcOHDyfXvfDCC5P1Q4fSv7+aP39+bm3Pnj3JdU9k7j7qfxT27EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBJeSbgNnnHFGsv7II48k6zfccENurdGlnovq7e1N1q+++urc2oQJE5Lr7t69O1mfNm1aoXo07NmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjG2dvA0qVLk/Wbb765RZ180TvvvJOsL1iwIFlP/Z595syZTfWE5rBnB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgGGdvA9dff31lr93f35+s79ixI1m/++67k/XUOHojqevdo3wN9+xmttbMBs1s14hlU83sOTPrzW6nVNsmgKLGchj/M0kLP7fsHklb3X2mpK3ZYwBtrGHY3X27pA8+t3ixpHXZ/XWSlpTcF4CSNfuZvdPdB7L7ByR15j3RzLokdTW5HQAlKXyCzt09NWGju/dI6pGY2BGoU7NDbwfNbLokZbeD5bUEoArNhn2zpBXZ/RWSni6nHQBVaXgYb2brJc2XNM3M9kn6kaSHJf3GzG6S9J6kb1fZ5MnulltuSda7utKnPLZs2ZJb6+vrS647OFjfQVlnZ+6pHlSgYdjdfVlO6Zsl9wKgQnxdFgiCsANBEHYgCMIOBEHYgSD4iWsbeP/995P17u7u1jTSYvPmzau7hVDYswNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzB7dq1apkvaOjI1k3s2TdPf/iRBdffHFy3UZeeumlZP3ll18u9PonG/bsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE4+wngNNOOy1Zv+iii3Jr9913X3LdRYsWNdXTsFNOSe8vjh8/3vRrDwwMJOsrV65M1o8dO9b0tk9G7NmBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjG2Vtg/Pjxyfrs2bOT9Q0bNiTr06dPz619+umnyXUbjWU3+s34woULk/VG3xFIGTduXLJ+3XXXJeuPPfZYbu3o0aNN9XQia7hnN7O1ZjZoZrtGLOs2s/1m9nr2V+ybGQAqN5bD+J9JGu2f739390uzv2fKbQtA2RqG3d23S/qgBb0AqFCRE3S3mdkb2WH+lLwnmVmXme00s50FtgWgoGbD/hNJX5N0qaQBSWvynujuPe4+193nNrktACVoKuzuftDdj7n7cUk/lXR5uW0BKFtTYTezkWM9SyXtynsugPZgqet6S5KZrZc0X9I0SQcl/Sh7fKkkl9Qv6bvunh6wHXqt9MZOUBMmTEjWG41Fb9y4sdD277///tzaCy+8kFz3xRdfTNanTp2arDd6/VmzZiXrVbrxxhtza5s2bUque+TIkbLbaRl3H/Vi/g2/VOPuy0ZZ/EThjgC0FF+XBYIg7EAQhB0IgrADQRB2IIiGQ2+lbuwEHnpL/Uz1gQceSK575513Ftr2s88+m6wvX748t/bRRx8l1z3zzDOT9WeeSf/Gac6cOcl66qekjz76aHLdRsN2ixcvTtZTnn/++WS9UW8ffvhh09uWpNdee63Q+il5Q2/s2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZM40uW/zggw/m1u64447kuh9//HGyfu+99ybr69evT9ZTY76XXXZZct3HH388WZ87N32Bob6+vmT91ltvza1t27Ytue7pp5+erF955ZXJeuonrtdee21y3Y6OjmS9kb179ybrM2bMKPT6KYyzA8ERdiAIwg4EQdiBIAg7EARhB4Ig7EAQjLNnUuPBUno8+pNPPkmu29XVlaxv2bIlWb/iiiuS9ZUrV+bWFi1KT7A7ceLEZL3Rb/WffPLJZL3ReHNdli0b7aLJ/y81Rj8Wt99+e7Le29tb6PVTGGcHgiPsQBCEHQiCsANBEHYgCMIOBEHYgSAYZ88MDKRnnE5dX73R9L579uxJ1hv9dvr8889P1ovo7u5O1h966KFk/dixYyV2gzI0Pc5uZuea2TYze9vM3jKz72fLp5rZc2bWm91OKbtpAOUZy2H8Z5J+4O5fl/QNSd8zs69LukfSVnefKWlr9hhAm2oYdncfcPdXs/uHJe2WdI6kxZLWZU9bJ2lJVU0CKO4rX+bJZnaepNmSfi+p092HP+gekNSZs06XpPSXwwFUbsxn481skqQNkla7+59G1nzoLN+oJ9/cvcfd57p7+sqFACo1prCb2XgNBf2X7r4xW3zQzKZn9emSBqtpEUAZGh7Gm5lJekLSbnf/8YjSZkkrJD2c3T5dSYctcuDAgWQ9NfR26qmnJte95JJLmuppWKNpk7dv355b27RpU3Ld/v7+ZJ2htZPHWD6z/62kf5T0ppm9ni37oYZC/hszu0nSe5K+XU2LAMrQMOzu/l+SRh2kl/TNctsBUBW+LgsEQdiBIAg7EARhB4Ig7EAQ/MQ1M3ny5GR9yZL8r/7PmTMnue7gYPr7RmvXrk3WU1MyS9LRo0eTdcTCpaSB4Ag7EARhB4Ig7EAQhB0IgrADQRB2IAjG2YGTDOPsQHCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EETDsJvZuWa2zczeNrO3zOz72fJuM9tvZq9nf4uqbxdAsxpevMLMpkua7u6vmtlkSa9IWqKh+dj/7O7/NuaNcfEKoHJ5F68Yy/zsA5IGsvuHzWy3pHPKbQ9A1b7UZ3YzO0/SbEm/zxbdZmZvmNlaM5uSs06Xme00s52FOgVQyJivQWdmkyT9h6QH3X2jmXVKOiTJJf2rhg71/7nBa3AYD1Qs7zB+TGE3s/GSfivpd+7+41Hq50n6rbvPavA6hB2oWNMXnDQzk/SEpN0jg56duBu2VNKuok0CqM5YzsZfJek/Jb0p6Xi2+IeSlkm6VEOH8f2SvpudzEu9Fnt2oGKFDuPLQtiB6nHdeCA4wg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBANLzhZskOS3hvxeFq2rB21a2/t2pdEb80qs7e/ziu09PfsX9i42U53n1tbAwnt2lu79iXRW7Na1RuH8UAQhB0Iou6w99S8/ZR27a1d+5LorVkt6a3Wz+wAWqfuPTuAFiHsQBC1hN3MFprZH8ysz8zuqaOHPGbWb2ZvZtNQ1zo/XTaH3qCZ7RqxbKqZPWdmvdntqHPs1dRbW0zjnZhmvNb3ru7pz1v+md3Mxkn6o6QFkvZJ2iFpmbu/3dJGcphZv6S57l77FzDM7O8k/VnSz4en1jKzRyV94O4PZ/9QTnH3u9ukt259yWm8K+otb5rxf1KN712Z0583o449++WS+tz9XXc/KunXkhbX0Efbc/ftkj743OLFktZl99dp6H+WlsvprS24+4C7v5rdPyxpeJrxWt+7RF8tUUfYz5G0d8TjfWqv+d5d0hYze8XMuupuZhSdI6bZOiCps85mRtFwGu9W+tw0423z3jUz/XlRnKD7oqvcfY6kf5D0vexwtS350Gewdho7/Ymkr2loDsABSWvqbCabZnyDpNXu/qeRtTrfu1H6asn7VkfY90s6d8Tjr2bL2oK7789uByU9paGPHe3k4PAMutntYM39/B93P+jux9z9uKSfqsb3LptmfIOkX7r7xmxx7e/daH216n2rI+w7JM00sxlmNkHSdyRtrqGPLzCzjuzEicysQ9K31H5TUW+WtCK7v0LS0zX28hfaZRrvvGnGVfN7V/v05+7e8j9JizR0Rv4dSf9SRw85ff2NpP/O/t6quzdJ6zV0WPc/Gjq3cZOkv5K0VVKvpOclTW2j3n6hoam939BQsKbX1NtVGjpEf0PS69nforrfu0RfLXnf+LosEAQn6IAgCDsQBGEHgiDsQBCEHQiCsANBEHYgiP8F9xRyWhvJGyQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[5])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(x, y, e, labelflip_proba=.25):\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    \n",
    "    y = np.logical_xor(\n",
    "        y,\n",
    "        (np.random.random(size=len(y)) < labelflip_proba).astype(int)\n",
    "    ).astype(int)\n",
    "    \n",
    "    color = np.logical_xor(\n",
    "        y,\n",
    "        (np.random.random(size=len(y)) < e).astype(int)\n",
    "    )\n",
    "    \n",
    "    x[color, :, :, 2] = 0\n",
    "    x[color, :, :, 1] = 0\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_env(x_train, y_train, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f36951f8f50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAN9ElEQVR4nO3da6xVdXrH8d+vOL6QUZGaHgmjZTAGM1SLDWJjSR1jGC/R6IlmMpgYG7HMCzBO0pAa+mI0DYZUmEaNmcBEHWxGzSRqgMmkavFCGxPiEVER6miNZsAj1CDKEC8Fnr44C3NGz/7vw95rXzjP95Ps7L3Xs9deT1b4sdZel/N3RAjAxPcnvW4AQHcQdiAJwg4kQdiBJAg7kMQJ3VyYbQ79Ax0WER5reltbdttX2H7L9ju272jnuwB0lls9z257kqTfSVogaZeklyUtjIgdhXnYsgMd1okt+zxJ70TEuxHxpaTHJV3bxvcB6KB2wj5d0u9Hvd9VTfsjthfbHrI91MayALSp4wfoImKtpLUSu/FAL7WzZd8t6cxR779TTQPQh9oJ+8uSzrH9XdsnSvqRpA31tAWgbi3vxkfEIdtLJT0taZKkhyLizdo6A1Crlk+9tbQwfrMDHdeRi2oAHD8IO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLlIZtxfJg0aVKxfuqpp3Z0+UuXLm1YO+mkk4rzzpo1q1hfsmRJsb5q1aqGtYULFxbn/fzzz4v1lStXFut33XVXsd4LbYXd9nuSDkg6LOlQRMytoykA9atjy35pRHxUw/cA6CB+swNJtBv2kPSM7VdsLx7rA7YX2x6yPdTmsgC0od3d+PkRsdv2n0l61vZ/R8Tm0R+IiLWS1kqS7WhzeQBa1NaWPSJ2V897JT0laV4dTQGoX8thtz3Z9slHX0v6gaTtdTUGoF7t7MYPSHrK9tHveTQi/r2WriaYs846q1g/8cQTi/WLL764WJ8/f37D2pQpU4rzXn/99cV6L+3atatYv++++4r1wcHBhrUDBw4U533ttdeK9RdffLFY70cthz0i3pX0lzX2AqCDOPUGJEHYgSQIO5AEYQeSIOxAEo7o3kVtE/UKugsuuKBY37RpU7He6dtM+9WRI0eK9VtuuaVYP3jwYMvL/uCDD4r1jz/+uFh/6623Wl52p0WEx5rOlh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuA8ew2mTp1arG/ZsqVYnzlzZp3t1KpZ7/v37y/WL7300oa1L7/8sjhv1usP2sV5diA5wg4kQdiBJAg7kARhB5Ig7EAShB1IgiGba7Bv375ifdmyZcX61VdfXay/+uqrxXqzP6lcsm3btmJ9wYIFxXqze8pnz57dsHb77bcX50W92LIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBLcz94HTjnllGK92fDCa9asaVhbtGhRcd6bbrqpWH/00UeLdfSflu9nt/2Q7b22t4+aNtX2s7bfrp5Pq7NZAPUbz278LyVd8bVpd0jaFBHnSNpUvQfQx5qGPSI2S/r69aDXSlpXvV4n6bqa+wJQs1avjR+IiOHq9YeSBhp90PZiSYtbXA6AmrR9I0xEROnAW0SslbRW4gAd0EutnnrbY3uaJFXPe+trCUAntBr2DZJurl7fLGl9Pe0A6JSmu/G2H5P0fUmn294l6aeSVkr6te1Fkt6X9MNONjnRffrpp23N/8knn7Q876233lqsP/7448V6szHW0T+ahj0iFjYoXVZzLwA6iMtlgSQIO5AEYQeSIOxAEoQdSIJbXCeAyZMnN6xt3LixOO8ll1xSrF955ZXF+jPPPFOso/sYshlIjrADSRB2IAnCDiRB2IEkCDuQBGEHkuA8+wR39tlnF+tbt24t1vfv31+sP//888X60NBQw9oDDzxQnLeb/zYnEs6zA8kRdiAJwg4kQdiBJAg7kARhB5Ig7EASnGdPbnBwsFh/+OGHi/WTTz655WUvX768WH/kkUeK9eHh4WI9K86zA8kRdiAJwg4kQdiBJAg7kARhB5Ig7EASnGdH0XnnnVesr169uli/7LLWB/tds2ZNsb5ixYpifffu3S0v+3jW8nl22w/Z3mt7+6hpd9rebXtb9biqzmYB1G88u/G/lHTFGNP/NSLmVI/f1tsWgLo1DXtEbJa0rwu9AOigdg7QLbX9erWbf1qjD9lebHvIduM/Rgag41oN+88lnS1pjqRhSQ2P0kTE2oiYGxFzW1wWgBq0FPaI2BMRhyPiiKRfSJpXb1sA6tZS2G1PG/V2UNL2Rp8F0B+anme3/Zik70s6XdIeST+t3s+RFJLek/TjiGh6czHn2SeeKVOmFOvXXHNNw1qze+XtMU8Xf+W5554r1hcsWFCsT1SNzrOfMI4ZF44x+cG2OwLQVVwuCyRB2IEkCDuQBGEHkiDsQBLc4oqe+eKLL4r1E04onyw6dOhQsX755Zc3rL3wwgvFeY9n/ClpIDnCDiRB2IEkCDuQBGEHkiDsQBKEHUii6V1vyO38888v1m+44YZi/cILL2xYa3YevZkdO3YU65s3b27r+ycatuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATn2Se4WbNmFeu33XZbsT44OFisn3HGGcfc03gdPny4WB8eLv/18iNHjtTZznGPLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMF59uNAs3PZN954Y8PakiVLivPOmDGjlZZqMTQ0VKyvWLGiWN+wYUOd7Ux4Tbfsts+0/bztHbbftH17NX2q7Wdtv109n9b5dgG0ajy78Yck/UNEfE/SX0taYvt7ku6QtCkizpG0qXoPoE81DXtEDEfE1ur1AUk7JU2XdK2kddXH1km6rlNNAmjfMf1mtz1D0gWStkgaiIijFyd/KGmgwTyLJS1uvUUAdRj30Xjb35b0hKSfRMSno2sxMjrkmIM2RsTaiJgbEXPb6hRAW8YVdtvf0kjQfxURT1aT99ieVtWnSdrbmRYB1KHpbrxtS3pQ0s6I+Nmo0gZJN0taWT2v70iHE8DAwJi/cL4ye/bsYv3+++8v1s8999xj7qkuW7ZsKdbvueeehrX168v/ZLhFtV7j+c3+N5JukvSG7W3VtOUaCfmvbS+S9L6kH3amRQB1aBr2iPgvSWMO7i7psnrbAdApXC4LJEHYgSQIO5AEYQeSIOxAEtziOk5Tp05tWFuzZk1x3jlz5hTrM2fObKmnOrz00kvF+urVq4v1p59+ulj/7LPPjrkndAZbdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IIs159osuuqhYX7ZsWbE+b968hrXp06e31FNdSuey77333uK8d999d7F+8ODBlnpC/2HLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJpDnPPjg42Fa9HTt37izWN27cWKwfPny4WF+1alXD2v79+4vzIg+27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQhCOi/AH7TEmPSBqQFJLWRsS9tu+U9PeS/rf66PKI+G2T7yovDEDbImLMUZfHE/ZpkqZFxFbbJ0t6RdJ1GhmP/Q8R0fiKjm9+F2EHOqxR2MczPvuwpOHq9QHbOyX19k+zADhmx/Sb3fYMSRdI2lJNWmr7ddsP2T6twTyLbQ/ZHmqrUwBtabob/9UH7W9LelHSioh40vaApI808jv+nzWyq39Lk+9gNx7osJZ/s0uS7W9J+o2kpyPiZ2PUZ0j6TUT8RZPvIexAhzUKe9PdeNuW9KCknaODXh24O2pQ0vZ2mwTQOeM5Gj9f0n9KekPSkWryckkLJc3RyG78e5J+XB3MK30XW3agw9raja8LYQc6r+XdeAATA2EHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJbg/Z/JGk90e9P72a1o/6tbd+7Uuit1bV2dufNyp09X72byzcHoqIuT1roKBfe+vXviR6a1W3emM3HkiCsANJ9Drsa3u8/JJ+7a1f+5LorVVd6a2nv9kBdE+vt+wAuoSwA0n0JOy2r7D9lu13bN/Rix4asf2e7Tdsb+v1+HTVGHp7bW8fNW2q7Wdtv109jznGXo96u9P27mrdbbN9VY96O9P287Z32H7T9u3V9J6uu0JfXVlvXf/NbnuSpN9JWiBpl6SXJS2MiB1dbaQB2+9JmhsRPb8Aw/bfSvqDpEeODq1l+18k7YuIldV/lKdFxD/2SW936hiH8e5Qb42GGf879XDd1Tn8eSt6sWWfJ+mdiHg3Ir6U9Lika3vQR9+LiM2S9n1t8rWS1lWv12nkH0vXNeitL0TEcERsrV4fkHR0mPGerrtCX13Ri7BPl/T7Ue93qb/Gew9Jz9h+xfbiXjczhoFRw2x9KGmgl82Moekw3t30tWHG+2bdtTL8ebs4QPdN8yPiryRdKWlJtbval2LkN1g/nTv9uaSzNTIG4LCk1b1sphpm/AlJP4mIT0fXernuxuirK+utF2HfLenMUe+/U03rCxGxu3reK+kpjfzs6Cd7jo6gWz3v7XE/X4mIPRFxOCKOSPqFerjuqmHGn5D0q4h4sprc83U3Vl/dWm+9CPvLks6x/V3bJ0r6kaQNPejjG2xPrg6cyPZkST9Q/w1FvUHSzdXrmyWt72Evf6RfhvFuNMy4erzuej78eUR0/SHpKo0ckf8fSf/Uix4a9DVT0mvV481e9ybpMY3s1v2fRo5tLJL0p5I2SXpb0n9ImtpHvf2bRob2fl0jwZrWo97ma2QX/XVJ26rHVb1ed4W+urLeuFwWSIIDdEAShB1IgrADSRB2IAnCDiRB2IEkCDuQxP8D13pxoJiMbBUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3695163a50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAM9UlEQVR4nO3df+xd9V3H8ddLBi4WNK242jAiOJvohhmQik6JmVmYiIkdJuIat9SJfhczsmH8MZyJq5pFXGTG+MeSLtRVM1mWAKNZzAarxLppln5BhELZ2pESWksbgmadmiHl7R/31HxXvt/z/n7vOeee276fj+Sbe+9533PPOxdePeeez73n44gQgPPfd4zdAIDZIOxAEYQdKIKwA0UQdqCI18xyY7Y59Q8MLCK83PJOe3bbN9r+qu3Dtu/o8loAhuVpx9ltXyDpa5JukHRU0n5J2yLiqZZ12LMDAxtiz36dpMMR8UxEvCTp05K2dng9AAPqEvbLJD235PHRZtm3sb1ge9H2YodtAeho8BN0EbFT0k6Jw3hgTF327MckXb7k8eubZQDmUJew75e02faVti+S9E5Je/ppC0Dfpj6Mj4iXbd8m6QuSLpC0KyKe7K0zAL2aeuhtqo3xmR0Y3CBfqgFw7iDsQBGEHSiCsANFEHagCMIOFEHYgSIIO1AEYQeKIOxAEYQdKIKwA0UQdqAIwg4UQdiBIgg7UARhB4og7EARhB0ogrADRRB2oAjCDhRB2IEiCDtQBGEHiiDsQBGEHSiCsANFEHagiKmnbAYk6ZKkfnFL7eeTdV+X1O9K6t9K6tV0CrvtI5JOSTot6eWI2NJHUwD618ee/Wci4oUeXgfAgPjMDhTRNewh6UHbj9heWO4JthdsL9pe7LgtAB10PYy/PiKO2X6dpIdsPx0R+5Y+ISJ2StopSbaj4/YATKnTnj0ijjW3JyXdL+m6PpoC0L+pw257ne1LztyX9HZJB/pqDEC/uhzGb5R0v+0zr/N3EfH5XrrCzFyZ1H8vqb8lqV+1hl7W6vuT+vsH3Pa5aOqwR8Qzkt7cYy8ABsTQG1AEYQeKIOxAEYQdKIKwA0U4YnZfauMbdMP44Zba7cm670rqr03qTurPtdROJev+SFLPfn311pba08m657KIWPY/C3t2oAjCDhRB2IEiCDtQBGEHiiDsQBGEHSiCS0nPge9J6n+W1H+5pZZd6rmrQ0n9Z1tqFyXrHkzql3asV8OeHSiCsANFEHagCMIOFEHYgSIIO1AEYQeKYJx9Dtyc1H99Jl0s7+tJ/Yak3vZ79s1r7AXdsGcHiiDsQBGEHSiCsANFEHagCMIOFEHYgSIYZ58DvzTgax9J6vuT+geTets4eqbtevfoX7pnt73L9knbB5Ys22D7IduHmtv1w7YJoKvVHMZ/UtKNZy27Q9LeiNgsaW/zGMAcS8MeEfskvXjW4q2Sdjf3d0t6R899AejZtJ/ZN0bE8eb+85I2rvRE2wuSFqbcDoCedD5BFxHRNmFjROyUtFNiYkdgTNMOvZ2wvUmSmtuT/bUEYAjThn2PpO3N/e2SHuinHQBDSQ/jbd+jyVTXl9o+KunDku6U9Bnbt0p6VtItQzZ5vvuNpJ6d8HiwpXY4WXfMQ7IVT/RgEGnYI2LbCqW39dwLgAHxdVmgCMIOFEHYgSIIO1AEYQeK4Ceuc+Dfk/qOWTQxgreM3UAx7NmBIgg7UARhB4og7EARhB0ogrADRRB2oAjG2Yt7f1Jfl9Sd1NsuTfSjybqZf07q/9Lx9c837NmBIgg7UARhB4og7EARhB0ogrADRRB2oAjG2c8B35XU39RS+8Nk3ZvW2MvZsr3FKx1e+3hSf09SP91h2+cj9uxAEYQdKIKwA0UQdqAIwg4UQdiBIgg7UATj7DNwYVK/Jqnfm9Q3tdT+J1k3G8vOfjN+Y1LPviPQ5oKk/otJ/S9bai+tsZfzQbpnt73L9knbB5Ys22H7mO3Hmr+u380AMLDVHMZ/Usv/A/4XEXF18/f3/bYFoG9p2CNin6QXZ9ALgAF1OUF3m+3Hm8P89Ss9yfaC7UXbix22BaCjacP+cUlvkHS1Jud47lrpiRGxMyK2RMSWKbcFoAdThT0iTkTE6Yh4RdInJF3Xb1sA+jZV2G0vHe25WdKBlZ4LYD44ou3K3pLteyS9VdKlkk5I+nDz+GpNLgt+RNJ7IyIbspXt9o2doy5K6tlY9H0dt/9HLbV/SNb9clLfkNSz178qqQ/pV1pqn03W/VafjcxYRCx7Of/0SzURsW2ZxXd37gjATPF1WaAIwg4UQdiBIgg7UARhB4pIh9563dg5PPTW9jPVP07W/d2O2/58Un9XS+0/k3W/L6lnv3C6Nqm3/ZT0o8m62bDd1qTe5otJPevtPzpsW5L+teP6bVYaemPPDhRB2IEiCDtQBGEHiiDsQBGEHSiCsANFMM7eyC5b/JGW2u8k6/5XUv/9pH5PUm8b8/2xZN2/SurZ5YUOJ/XfbKk9nKz73Un9J5N6209cfyFZd11SzzyX1K/s+PptGGcHiiPsQBGEHSiCsANFEHagCMIOFEHYgSIYZ2+0jQdL7ePR/52su5DUH0zqP57U39NSy6bXfW1Sz36r/9dJPRtvHstyl0xeqm2MfjV+K6kf6vj6bRhnB4oj7EARhB0ogrADRRB2oAjCDhRB2IEiGGdvZPNNt11fPZve9+mknv12+oeSehc7kvqfJvXTPfWB/kw9zm77ctsP237K9pO2P9As32D7IduHmtv1fTcNoD+rOYx/WdJvR8QbJf2EpPfZfqOkOyTtjYjNkvY2jwHMqTTsEXE8Ih5t7p+SdFDSZZrMvrO7edpuSe8YqkkA3b1mLU+2fYWkayR9RdLGiDjzUfd5SRtXWGdB+dfDAQxs1WfjbV8s6V5Jt0fEN5bWYnKWb9mTbxGxMyK2RER27UIAA1pV2G1fqEnQPxUR9zWLT9je1NQ3STo5TIsA+pAextu2pLslHYyIjy0p7ZG0XdKdze0Dg3Q4I88n9baht+9M1n3zGns5WzZt8r6W2meTdY8kdYbWzh+r+cz+U5LeLekJ2481yz6kScg/Y/tWSc9KumWYFgH0IQ17RHxJ0rKD9JLe1m87AIbC12WBIgg7UARhB4og7EARhB0ogp+4Ni5J6m1f/L82WTf7ttGupN42JbMkvZTUUQuXkgaKI+xAEYQdKIKwA0UQdqAIwg4UQdiBIhhnB84zjLMDxRF2oAjCDhRB2IEiCDtQBGEHiiDsQBGEHSiCsANFEHagCMIOFEHYgSIIO1AEYQeKIOxAEWnYbV9u+2HbT9l+0vYHmuU7bB+z/Vjzd9Pw7QKYVnrxCtubJG2KiEdtXyLpEU3mTLhF0jcj4s9XvTEuXgEMbqWLV6xmfvbjko4390/ZPijpsn7bAzC0NX1mt32FpGskfaVZdJvtx23vsr1+hXUWbC/aXuzUKYBOVn0NOtsXS/pHSR+JiPtsb5T0gqSQ9CeaHOr/WvIaHMYDA1vpMH5VYbd9oaTPSfpCRHxsmfoVkj4XEVclr0PYgYFNfcFJ25Z0t6SDS4PenLg742ZJB7o2CWA4qzkbf72kf5L0hKRXmsUfkrRN0tWaHMYfkfTe5mRe22uxZwcG1ukwvi+EHRge140HiiPsQBGEHSiCsANFEHagCMIOFEHYgSIIO1AEYQeKIOxAEYQdKIKwA0UQdqAIwg4UkV5wsmcvSHp2yeNLm2XzaF57m9e+JHqbVp+9/cBKhZn+nv1VG7cXI2LLaA20mNfe5rUvid6mNaveOIwHiiDsQBFjh33nyNtvM6+9zWtfEr1Naya9jfqZHcDsjL1nBzAjhB0oYpSw277R9ldtH7Z9xxg9rMT2EdtPNNNQjzo/XTOH3knbB5Ys22D7IduHmttl59gbqbe5mMa7ZZrxUd+7sac/n/lndtsXSPqapBskHZW0X9K2iHhqpo2swPYRSVsiYvQvYNj+aUnflPQ3Z6bWsv1RSS9GxJ3NP5TrI+KDc9LbDq1xGu+BeltpmvFf1YjvXZ/Tn09jjD37dZIOR8QzEfGSpE9L2jpCH3MvIvZJevGsxVsl7W7u79bkf5aZW6G3uRARxyPi0eb+KUlnphkf9b1r6Wsmxgj7ZZKeW/L4qOZrvveQ9KDtR2wvjN3MMjYumWbreUkbx2xmGek03rN01jTjc/PeTTP9eVecoHu16yPiWkk/J+l9zeHqXIrJZ7B5Gjv9uKQ3aDIH4HFJd43ZTDPN+L2Sbo+IbyytjfneLdPXTN63McJ+TNLlSx6/vlk2FyLiWHN7UtL9mnzsmCcnzsyg29yeHLmf/xcRJyLidES8IukTGvG9a6YZv1fSpyLivmbx6O/dcn3N6n0bI+z7JW22faXtiyS9U9KeEfp4FdvrmhMnsr1O0ts1f1NR75G0vbm/XdIDI/bybeZlGu+VphnXyO/d6NOfR8TM/yTdpMkZ+a9L+oMxelihrx+U9G/N35Nj9ybpHk0O6/5Xk3Mbt0r6Xkl7JR2S9EVJG+aot7/VZGrvxzUJ1qaRertek0P0xyU91vzdNPZ719LXTN43vi4LFMEJOqAIwg4UQdiBIgg7UARhB4og7EARhB0o4v8AMObNPw+OOV8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(28, 28, 3)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy', 'crossentropy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                294944    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 314,369\n",
      "Trainable params: 314,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2) checkÂ model is good for the initial task (greyscale + no label flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.0098 - accuracy: 0.9959 - crossentropy: 0.0098 - val_loss: 9.9567e-07 - val_accuracy: 1.0000 - val_crossentropy: 9.9567e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f36945c39d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model().fit(*get_env(x_train, y_train, 0),\n",
    "                batch_size=128,\n",
    "                epochs=1,\n",
    "                verbose=1,\n",
    "                validation_data=get_env(x_test, y_test, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â 0.3) our 3 envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = get_env(x_train[:10000], y_train[:10000], .1)\n",
    "e2 = get_env(x_train[10000:20000], y_train[10000:20000], .2)\n",
    "e3 = get_env(x_train[20000:30000], y_train[20000:30000], .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Claim:Â ERM in separate envs are fooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 3s 256us/step - loss: 0.3809 - accuracy: 0.8827 - crossentropy: 0.3809 - val_loss: 1.9649 - val_accuracy: 0.0957 - val_crossentropy: 1.9649\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 0.3392 - accuracy: 0.8996 - crossentropy: 0.3392 - val_loss: 1.7764 - val_accuracy: 0.0957 - val_crossentropy: 1.7764\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 0.3300 - accuracy: 0.8998 - crossentropy: 0.3300 - val_loss: 1.7693 - val_accuracy: 0.0961 - val_crossentropy: 1.7693\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 231us/step - loss: 0.3191 - accuracy: 0.8999 - crossentropy: 0.3191 - val_loss: 1.6670 - val_accuracy: 0.1075 - val_crossentropy: 1.6670\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 232us/step - loss: 0.3176 - accuracy: 0.8998 - crossentropy: 0.3176 - val_loss: 1.5179 - val_accuracy: 0.0957 - val_crossentropy: 1.5179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f36446d0850>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model().fit(*e1,\n",
    "                batch_size=128,\n",
    "                epochs=5,\n",
    "                verbose=1,\n",
    "                validation_data=e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 3s 254us/step - loss: 0.5339 - accuracy: 0.7781 - crossentropy: 0.5339 - val_loss: 1.2771 - val_accuracy: 0.1232 - val_crossentropy: 1.2771\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s 230us/step - loss: 0.4979 - accuracy: 0.7919 - crossentropy: 0.4979 - val_loss: 1.3912 - val_accuracy: 0.1009 - val_crossentropy: 1.3912\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 2s 231us/step - loss: 0.4774 - accuracy: 0.7925 - crossentropy: 0.4774 - val_loss: 1.0916 - val_accuracy: 0.3035 - val_crossentropy: 1.0916\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 0.4657 - accuracy: 0.7917 - crossentropy: 0.4657 - val_loss: 1.5232 - val_accuracy: 0.1373 - val_crossentropy: 1.5232\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 238us/step - loss: 0.4640 - accuracy: 0.7950 - crossentropy: 0.4640 - val_loss: 1.3337 - val_accuracy: 0.1258 - val_crossentropy: 1.3337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f369574efd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model().fit(*e2,\n",
    "                batch_size=128,\n",
    "                epochs=5,\n",
    "                verbose=1,\n",
    "                validation_data=e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â 2) Claim: ERM on merged envs is fooled too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "e12 = (np.vstack([e1[0], e2[0]]), np.hstack([e1[1], e2[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 5s 225us/step - loss: 0.4547 - accuracy: 0.8372 - crossentropy: 0.4547 - val_loss: 1.8169 - val_accuracy: 0.0957 - val_crossentropy: 1.8169\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 4s 210us/step - loss: 0.4150 - accuracy: 0.8468 - crossentropy: 0.4150 - val_loss: 1.1267 - val_accuracy: 0.0962 - val_crossentropy: 1.1267\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 4s 207us/step - loss: 0.4030 - accuracy: 0.8469 - crossentropy: 0.4030 - val_loss: 1.4850 - val_accuracy: 0.1054 - val_crossentropy: 1.4850\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 4s 211us/step - loss: 0.3966 - accuracy: 0.8462 - crossentropy: 0.3966 - val_loss: 1.4074 - val_accuracy: 0.1528 - val_crossentropy: 1.4074\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 4s 207us/step - loss: 0.3906 - accuracy: 0.8469 - crossentropy: 0.3906 - val_loss: 1.4231 - val_accuracy: 0.0971 - val_crossentropy: 1.4231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f36442cd2d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model().fit(*e12,\n",
    "                batch_size=128,\n",
    "                epochs=5,\n",
    "                verbose=1,\n",
    "                validation_data=e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â 3) Claim: Robust ERM will be fooled too\n",
    "\n",
    "robust objective: $f = argmin_f \\{Â max_e R^e(f) - var(Y^e) \\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3263 0.24992603999999996 0.07637396000000002\n",
      "0.457 0.24998479000000007 0.20701520999999995\n"
     ]
    }
   ],
   "source": [
    "e1_loss = .3263\n",
    "e2_loss = .4570\n",
    "e1_var = e1[1].var()\n",
    "e2_var = e2[1].var()\n",
    "print(e1_loss, e1_var, e1_loss - e1_var)\n",
    "print(e2_loss, e2_var, e2_loss - e2_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â 3.1) jittering implem of robust objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f363c79c210>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter_color(x, scale=.25):\n",
    "    x_jit = x.copy()\n",
    "    for channel in (0,): #range(3):\n",
    "        mask = x_jit[:,:,channel] > 0\n",
    "        x_jit[:,:,channel] += mask * (np.random.normal(scale=scale, size=(28,28)))**2\n",
    "        if x_jit[:,:,channel].max() > 1.:\n",
    "            x_jit[:,:,channel] /= x_jit[:,:,channel].max()\n",
    "    return x_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f36529e4750>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_jit = jitter_color(x_train[5])\n",
    "plt.imshow(x_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc_jit 0.84785 train_acc 0.8473 ood_acc 0.109\n",
      "train_acc_jit 0.84845 train_acc 0.8484 ood_acc 0.0961\n",
      "train_acc_jit 0.84845 train_acc 0.84785 ood_acc 0.1011\n",
      "train_acc_jit 0.84845 train_acc 0.84795 ood_acc 0.0993\n",
      "train_acc_jit 0.84285 train_acc 0.8413 ood_acc 0.1539\n",
      "train_acc_jit 0.8464 train_acc 0.8456 ood_acc 0.1221\n",
      "train_acc_jit 0.8488 train_acc 0.84805 ood_acc 0.1022\n",
      "train_acc_jit 0.84845 train_acc 0.84845 ood_acc 0.0959\n",
      "train_acc_jit 0.8486 train_acc 0.84835 ood_acc 0.1001\n",
      "train_acc_jit 0.84845 train_acc 0.84815 ood_acc 0.098\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    e12_jit = np.array([jitter_color(xx, scale=.5) for xx in e12[0]])\n",
    "    m = get_model()\n",
    "    m.fit(e12_jit, e12[1],\n",
    "          batch_size=128,\n",
    "          epochs=3,\n",
    "          verbose=0)\n",
    "    y_pred_train_jit = (m.predict(e12_jit)[:,0] > .5).astype(int)\n",
    "    y_pred_train = (m.predict(e12[0])[:,0] > .5).astype(int)\n",
    "    y_pred_ood = (m.predict(e3[0])[:,0] > .5).astype(int)\n",
    "    print(\"train_acc_jit\", accuracy_score(e12[1], y_pred_train_jit), \n",
    "          \"train_acc\", accuracy_score(e12[1], y_pred_train), \n",
    "          \"ood_acc\", accuracy_score(e3[1], y_pred_ood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Domain Adaptation: merged ERM with constraint on distrib_dist({phi(X^e)}_e=1..n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Adverserial Domain Adaptation\n",
    "http://openaccess.thecvf.com/content_cvpr_2017/papers/Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedEnvDataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, adv_weight=.05, e1_weight=1.0, e2_weight=1.0, batch_size=128, shuffle=True):\n",
    "        \n",
    "        self.__load_initial_data()\n",
    "        self.__create_envs()\n",
    "        self.__create_validation_envs()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.halfbatch_size = int(np.floor(batch_size/2))\n",
    "        self.e1_weight = e1_weight\n",
    "        self.e2_weight = e2_weight\n",
    "        self.adv_weight = adv_weight\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.e1[1]))\n",
    "        print(len(self), 'batches/epoch')\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.e1[1]) / self.halfbatch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        \n",
    "        halfbatch_indices = self.indices[index*self.halfbatch_size:(index+1)*self.halfbatch_size]\n",
    "        \n",
    "        e1_x_batch = self.e1[0][halfbatch_indices,:,:,:]\n",
    "        e2_x_batch = self.e2[0][halfbatch_indices,:,:,:]\n",
    "        e1_y_batch = self.e1[1][halfbatch_indices]\n",
    "        e2_y_batch = self.e2[1][halfbatch_indices]\n",
    "        \n",
    "        mixed_x = np.vstack([\n",
    "            e1_x_batch,\n",
    "            e2_x_batch,\n",
    "        ])\n",
    "        \n",
    "        e1_y = np.hstack([e1_y_batch, np.zeros(self.halfbatch_size)])\n",
    "        e1_w = self.e1_weight * np.hstack([np.ones(self.halfbatch_size), np.zeros(self.halfbatch_size)])\n",
    "        \n",
    "        e2_y = np.hstack([np.zeros(self.halfbatch_size), e2_y_batch])\n",
    "        e2_w = self.e2_weight * np.hstack([np.zeros(self.halfbatch_size), np.ones(self.halfbatch_size)])\n",
    "        \n",
    "        adv_y = np.hstack([np.ones(self.halfbatch_size), np.zeros(self.halfbatch_size)])\n",
    "        adv_w = self.adv_weight * np.ones(len(adv_y))\n",
    "        \n",
    "        return [\n",
    "            mixed_x,\n",
    "            [e1_y, e2_y, adv_y],\n",
    "            [e1_w, e2_w, adv_w]\n",
    "        ]\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.e1[1]))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __load_initial_data(self):\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        # convert to RGB\n",
    "        x_train = np.stack((x_train,)*3, axis=-1)\n",
    "        x_test = np.stack((x_test,)*3, axis=-1)\n",
    "\n",
    "        # normalize\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "\n",
    "        # binary label\n",
    "        y_train = (y_train < 5).astype(int)\n",
    "        y_test = (y_test < 5).astype(int)\n",
    "        \n",
    "        self.original_data = {\n",
    "            'x_train':x_train,\n",
    "            'x_test':x_test,\n",
    "            'y_train':y_train,\n",
    "            'y_test':y_test\n",
    "        }\n",
    "        \n",
    "    def __create_envs(self):\n",
    "        self.e1 = self.__create_env(self.original_data['x_train'][:10000], \n",
    "                                    self.original_data['y_train'][:10000], .1)\n",
    "        self.e2 = self.__create_env(self.original_data['x_train'][10000:20000], \n",
    "                                    self.original_data['y_train'][10000:20000], .2)\n",
    "        self.e3 = self.__create_env(self.original_data['x_train'][20000:30000], \n",
    "                                    self.original_data['y_train'][20000:30000], .9)\n",
    "        \n",
    "    def __create_validation_envs(self):\n",
    "        self.e11 = self.__create_env(self.original_data['x_train'][30000:40000], \n",
    "                                     self.original_data['y_train'][30000:40000], .1)\n",
    "        self.e22 = self.__create_env(self.original_data['x_train'][40000:50000], \n",
    "                                     self.original_data['y_train'][40000:50000], .2)\n",
    "        self.e33 = self.__create_env(self.original_data['x_train'][50000:60000], \n",
    "                                     self.original_data['y_train'][50000:60000], .9)\n",
    "        half_len = int(len(self.e11[1])/2)\n",
    "        self.eaa = [\n",
    "            np.vstack([self.e11[0][:half_len], self.e22[0][:half_len]]),\n",
    "            np.hstack([np.ones(half_len), np.zeros(half_len)])\n",
    "        ]\n",
    "    \n",
    "    def __create_env(self, x, y, e, labelflip_proba=.25):\n",
    "        x = x.copy()\n",
    "        y = y.copy()\n",
    "\n",
    "        y = np.logical_xor(\n",
    "            y,\n",
    "            (np.random.random(size=len(y)) < labelflip_proba).astype(int)\n",
    "        ).astype(int)\n",
    "\n",
    "        color = np.logical_xor(\n",
    "            y,\n",
    "            (np.random.random(size=len(y)) < e).astype(int)\n",
    "        )\n",
    "\n",
    "        x[color, :, :, 2] = 0\n",
    "        x[color, :, :, 1] = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ada_mixed_model():\n",
    "    \n",
    "    input_images = Input(shape=(28, 28, 3))\n",
    "    \n",
    "    cnn = Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu')(input_images)\n",
    "    cnn = Conv2D(64, (3, 3), activation='relu')(cnn)\n",
    "    cnn = MaxPooling2D(pool_size=(2, 2))(cnn)\n",
    "    cnn = Dropout(0.25)(cnn)\n",
    "    cnn = Flatten()(cnn)\n",
    "    \n",
    "    env1 = Dense(32, activation='relu')(cnn)\n",
    "    env1 = Dropout(0.5)(env1)\n",
    "    env1 = Dense(1, activation='sigmoid', name='env1')(env1)\n",
    "    \n",
    "    env2 = Dense(32, activation='relu')(cnn)\n",
    "    env2 = Dropout(0.5)(env2)\n",
    "    env2 = Dense(1, activation='sigmoid', name='env2')(env2)\n",
    "        \n",
    "    adv = Dense(32, activation='relu')(cnn)\n",
    "    adv = Dropout(0.5)(adv)\n",
    "    adv = Dense(1, activation='sigmoid', name='adv')(adv)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[input_images],\n",
    "        outputs=[env1, env2, adv]\n",
    "    )\n",
    "    \n",
    "    def adv_loss(y_true, y_pred):\n",
    "        return - keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    model.compile(\n",
    "        loss=[\n",
    "            keras.losses.binary_crossentropy,\n",
    "            keras.losses.binary_crossentropy,\n",
    "            adv_loss\n",
    "        ],\n",
    "        optimizer=keras.optimizers.Adadelta(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 batches/epoch\n",
      "  0 |e1->e11 acc: 0.901 | e2->e22 acc: 0.801 | e_a->eaa acc: 0.501 | e1->e3 acc: 0.101 e2->e3 acc: 0.141\n",
      "  1 |e1->e11 acc: 0.900 | e2->e22 acc: 0.798 | e_a->eaa acc: 0.501 | e1->e3 acc: 0.104 e2->e3 acc: 0.155\n",
      "  2 |e1->e11 acc: 0.900 | e2->e22 acc: 0.788 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.108 e2->e3 acc: 0.257\n",
      "  3 |e1->e11 acc: 0.897 | e2->e22 acc: 0.792 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.119 e2->e3 acc: 0.266\n",
      "  4 |e1->e11 acc: 0.894 | e2->e22 acc: 0.794 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.123 e2->e3 acc: 0.257\n",
      "  5 |e1->e11 acc: 0.890 | e2->e22 acc: 0.789 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.145 e2->e3 acc: 0.276\n",
      "  6 |e1->e11 acc: 0.888 | e2->e22 acc: 0.786 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.162 e2->e3 acc: 0.307\n",
      "  7 |e1->e11 acc: 0.502 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      "  8 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      "  9 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 10 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 11 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 12 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 13 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 14 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 15 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 16 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 17 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 18 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 19 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 20 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 21 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 22 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 23 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 24 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 25 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 26 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 27 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 28 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 29 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 30 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 31 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 32 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 33 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 34 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 35 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 36 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 37 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 38 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 39 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 40 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 41 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 42 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 43 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 44 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 45 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 46 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 47 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 48 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 49 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 50 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 51 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 52 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 53 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 54 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 55 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 56 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 57 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 58 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 59 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 60 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 61 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 62 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 63 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 64 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 65 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 66 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 67 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 68 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 69 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 70 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 71 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 72 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 73 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 74 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 75 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 76 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 77 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 79 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 80 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 81 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 82 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 83 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 84 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 85 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 86 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 87 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 88 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 89 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 90 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 91 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 92 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 93 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 94 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 95 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 96 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 97 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 98 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n",
      " 99 |e1->e11 acc: 0.501 | e2->e22 acc: 0.512 | e_a->eaa acc: 0.500 | e1->e3 acc: 0.508 e2->e3 acc: 0.508\n"
     ]
    }
   ],
   "source": [
    "m = get_ada_mixed_model()\n",
    "g = MixedEnvDataGenerator(adv_weight=5e-4, e1_weight=1, e2_weight=1, shuffle=True)\n",
    "logs = defaultdict(list)\n",
    "for _ in range(100):\n",
    "    m.fit_generator(\n",
    "        g,\n",
    "        epochs=5,\n",
    "        verbose=0,\n",
    "    )\n",
    "    print('%3d |'%_, end='')\n",
    "    (\n",
    "        loss_all, \n",
    "        e1_loss, e2_loss, adv_loss,\n",
    "        e1_acc, e2_acc, e3_acc\n",
    "    ) = m.evaluate(g.e11[0], [g.e11[1],g.e11[1],g.e11[1]], verbose=0)\n",
    "    logs['e11acc'] += [e1_acc]\n",
    "    logs['e11loss'] += [e1_loss]\n",
    "    print('e1->e11 acc: %.3f'%e1_acc, end=' | ')\n",
    "    (\n",
    "        loss_all, \n",
    "        e1_loss, e2_loss, adv_loss,\n",
    "        e1_acc, e2_acc, e3_acc\n",
    "    ) = m.evaluate(g.e22[0], [g.e22[1],g.e22[1],g.e22[1]], verbose=0)\n",
    "    logs['e22acc'] += [e2_acc]\n",
    "    logs['e22loss'] += [e2_loss]\n",
    "    print('e2->e22 acc: %.3f'%e2_acc, end=' | ')\n",
    "    (\n",
    "        loss_all, \n",
    "        e1_loss, e2_loss, adv_loss,\n",
    "        e1_acc, e2_acc, adv_acc\n",
    "    ) = m.evaluate(g.eaa[0], [g.eaa[1],g.eaa[1],g.eaa[1]], verbose=0)\n",
    "    logs['eaaacc'] += [adv_acc]\n",
    "    logs['eaaloss'] += [adv_loss]\n",
    "    print('e_a->eaa acc: %.3f'%adv_acc, end=' | ')\n",
    "    (\n",
    "        loss_all, \n",
    "        e1_loss, e2_loss, adv_loss,\n",
    "        e1_acc, e2_acc, e3_acc\n",
    "    ) = m.evaluate(g.e3[0], [g.e3[1], g.e3[1], g.e3[1]], verbose=0)\n",
    "    logs['e1_33acc'] += [e1_acc]\n",
    "    logs['e1_33loss'] += [e1_loss]\n",
    "    logs['e2_33acc'] += [e2_acc]\n",
    "    logs['e2_33loss'] += [e2_loss]\n",
    "    print('e1->e3 acc: %.3f'%e1_acc, 'e2->e3 acc: %.3f'%e2_acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in ('e11acc','e22acc',\n",
    "          'eaaacc','e1_33acc','e2_33acc'):\n",
    "    plt.plot(logs[k[:-3]+'loss'], label=k[:-3]+'loss')\n",
    "plt.legend(loc='center left')\n",
    "plt.grid()\n",
    "plt.yscale('symlog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k in ('e11acc','e22acc','eaaacc','e1_33acc','e2_33acc'):\n",
    "    plt.plot(logs[k], label=k)\n",
    "plt.legend(loc='best')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â 4) Invariant Risk Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer model_40 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]],\n\n\n       [[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]],\n\n\n       [[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]],\n\n\n       ...,\n\n\n       [[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]],\n\n\n       [[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]],\n\n\n       [[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]]], dtype=float32)]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    696\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[0;32m--> 697\u001b[0;31m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'numpy.ndarray'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-e836cbfc7c7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-327-e836cbfc7c7f>\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(model, inputs, targets)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-327-e836cbfc7c7f>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(model, x, y, training)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# training=training is needed only if there are layers with different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# behavior during training versus inference (e.g. Dropout).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# with the input_spec set at build time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;31m# Handle mask propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                                  \u001b[0;34m'Received type: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full input: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. All inputs to the layer '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                                  'should be tensors.')\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer model_40 was called with an input that isn't a symbolic tensor. Received type: <class 'numpy.ndarray'>. Full input: [array([[[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]],\n\n\n       [[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]],\n\n\n       [[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]],\n\n\n       ...,\n\n\n       [[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]],\n\n\n       [[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]],\n\n\n       [[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        ...,\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]]], dtype=float32)]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "def loss(model, x, y, training):\n",
    "    # training=training is needed only if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    y_ = model(x, training=training)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, training=True)\n",
    "        return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "    for i in range(len(dg)):\n",
    "        x, y, y2 = dg[i]\n",
    "        # Optimize the model\n",
    "        loss_value, grads = grad(model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Track progress\n",
    "        epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "        # Compare predicted label to actual label\n",
    "        # training=True is needed only if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        epoch_accuracy.update_state(y, model(x, training=True))\n",
    "\n",
    "    dg.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRMLoss(object):\n",
    "    \n",
    "    def __init__(self, model, lambda_=1):\n",
    "        self.lambda_=lambda_\n",
    "        self.model = model\n",
    "        self.penalty = 0\n",
    "    \n",
    "    @classmethod\n",
    "    def get_final_layer_gradients(clz, model, x, y):\n",
    "        output_tensors = model.optimizer.get_gradients(model.total_loss, \n",
    "                                                       #tf.Variable([1.])) \n",
    "                                                       model.layers[-1].weights[1])\n",
    "        input_tensors = model.inputs + model.sample_weights[0] + model.targets[0] + [K.learning_phase()]\n",
    "        get_gradients = K.function(inputs=input_tensors, outputs=output_tensors)\n",
    "        inputs = [\n",
    "            x, \n",
    "            np.ones(y.shape), \n",
    "            y, \n",
    "            0\n",
    "        ]\n",
    "        return get_gradients(inputs)\n",
    "    \n",
    "    def on_batch_end(self, x, y):\n",
    "        loss, acc = self.model.evaluate(x,y,verbose=0)\n",
    "        grads = IRMLoss.get_final_layer_gradients(self.model, x, y)\n",
    "        self.penalty = np.sum([self.loss * g * g for g in grads[0]])\n",
    "           \n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        loss = keras.losses.binary_crossentropy(y_true, y_pred) + self.lambda_ * self.penalty\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irm_model(lambda_=100):\n",
    "    \n",
    "    input_images = Input(shape=(28, 28, 3))\n",
    "    \n",
    "    cnn = Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu')(input_images)\n",
    "    cnn = Conv2D(64, (3, 3), activation='relu')(cnn)\n",
    "    cnn = MaxPooling2D(pool_size=(2, 2))(cnn)\n",
    "    cnn = Dropout(0.25)(cnn)\n",
    "    cnn = Flatten()(cnn)\n",
    "    \n",
    "    spe = Dense(32, activation='relu')(cnn)\n",
    "    spe = Dropout(0.5)(spe)    \n",
    "    spe = Dense(1, activation='sigmoid', name='w')(spe)\n",
    "    \n",
    "    dummy = Dense(1, activation='sigmoid', use_bias=True, \n",
    "                  bias_initializer=keras.initializers.Ones(),\n",
    "                  kernel_initializer=keras.initializers.Zeros(),\n",
    "                  name='w1', trainable=False)(spe)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[input_images],\n",
    "        outputs=[spe, dummy]\n",
    "    )  \n",
    "\n",
    "    model.compile(\n",
    "        loss=[\n",
    "            IRMLoss(model, lambda_),\n",
    "            keras.losses.binary_crossentropy\n",
    "        ],\n",
    "        optimizer=keras.optimizers.Adadelta(),\n",
    "        metrics=['accuracy'],\n",
    "    )    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_irm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'w1_9/bias:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.layers[-1].weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "ename": "_SymbolicException",
     "evalue": "Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'strided_slice_23:0' shape=(1,) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: strided_slice_23:0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31m_SymbolicException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-323-567f4594854a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mIRMLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_final_layer_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-314-da877cba82b3>\u001b[0m in \u001b[0;36mget_final_layer_gradients\u001b[0;34m(clz, model, x, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         ]\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     73\u001b[0m       raise core._SymbolicException(\n\u001b[1;32m     74\u001b[0m           \u001b[0;34m\"Inputs to eager execution function cannot be Keras symbolic \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m           \"tensors, but found {}\".format(keras_symbolic_tensors))\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_SymbolicException\u001b[0m: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'strided_slice_23:0' shape=(1,) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "IRMLoss.get_final_layer_gradients(m, e1[0], e1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 batches/epoch\n"
     ]
    }
   ],
   "source": [
    "g = MixedEnvDataGenerator(adv_weight=5e-4, e1_weight=1, e2_weight=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "        \n",
    "    input_images = Input(shape=(28, 28, 3))\n",
    "    \n",
    "    cnn = Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu')(input_images)\n",
    "    cnn = Conv2D(64, (3, 3), activation='relu')(cnn)\n",
    "    cnn = MaxPooling2D(pool_size=(2, 2))(cnn)\n",
    "    cnn = Dropout(0.25)(cnn)\n",
    "    cnn = Flatten()(cnn)\n",
    "    \n",
    "    spe = Dense(32, activation='relu')(cnn)\n",
    "    spe = Dropout(0.5)(spe)\n",
    "    \n",
    "    # IRM part\n",
    "    \n",
    "    w1 = np.zeros(32)\n",
    "    w1[0] = 1    \n",
    "    irm = Dense(1, activation='sigmoid', name='env', \n",
    "                bias_initializer=keras.initializers.Zeros(),\n",
    "                kernel_initializer=keras.initializers.Constant(value=w1),\n",
    "                use_bias=False,\n",
    "                trainable=False)(spe)\n",
    "    irm_model = Model(\n",
    "        inputs=[input_images],\n",
    "        outputs=[irm]\n",
    "    )\n",
    "    \n",
    "    class IRMLoss(object):\n",
    "        def __init__(self, model, lambda_=1):\n",
    "            self.lambda_=lambda_\n",
    "            self.dummy_grad_norm = 0\n",
    "            self.loss = 1\n",
    "            self.accuracy = 0\n",
    "            self.penalty = 1\n",
    "            \n",
    "        def update(self, model, x, y):\n",
    "            \n",
    "            self.loss, self.accuracy = model.evaluate(x,y,verbose=0)\n",
    "\n",
    "            output_tensors = model.optimizer.get_gradients(model.total_loss, model.layers[-1].weights[0])\n",
    "            input_tensors = model.inputs + model.sample_weights + model.targets + [K.learning_phase()]\n",
    "            get_gradients = K.function(inputs=input_tensors, outputs=output_tensors)\n",
    "            inputs = [x, np.ones(len(y)), y, 0]\n",
    "            grads = get_gradients(inputs)\n",
    "            self.dummy_grad_norm = np.sqrt(np.sum([_**2 for _ in grads[0]]))\n",
    "            \n",
    "            self.penalty = np.sum([self.loss * _**2 for _ in grads[0]])\n",
    "            \n",
    "        def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "            loss = keras.losses.binary_crossentropy(y_true, y_pred) + self.lambda_ * self.penalty\n",
    "            #if loss > 1:\n",
    "            #    return loss / self.lambda_\n",
    "            return loss\n",
    "    \n",
    "    irm_loss = IRMLoss()\n",
    "    \n",
    "    irm_model.compile(\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        optimizer=keras.optimizers.Adadelta(),\n",
    "        metrics=['accuracy']\n",
    "    )    \n",
    "    \n",
    "    if True:\n",
    "        baseline = Dense(1, activation='sigmoid', name='base', \n",
    "                         use_bias=False,\n",
    "                         trainable=True)(spe)\n",
    "        baseline_model = Model(\n",
    "            inputs=input_images,\n",
    "            outputs=baseline\n",
    "        )\n",
    "        baseline_model.compile(\n",
    "            loss=irm_loss,\n",
    "            optimizer=keras.optimizers.Adadelta(),\n",
    "            metrics=['accuracy']\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irm_loss.lambda_*irm_loss.penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 1000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5\n",
      "--------------------------------------------------------------------------------\n",
      "lam:      0 | pen: 0.185 loss: 1.309 acc: 0.899 | pen: 0.116 loss: 1.482 acc: 0.799 | e3loss: 2.245 e3acc: 0.108\n",
      "lam:      0 | pen: 0.060 loss: 1.350 acc: 0.897 | pen: 0.130 loss: 1.459 acc: 0.799 | e3loss: 2.261 e3acc: 0.113\n",
      "lam:      0 | pen: 0.117 loss: 1.342 acc: 0.896 | pen: 0.187 loss: 1.471 acc: 0.799 | e3loss: 2.205 e3acc: 0.100\n",
      "lam:      0 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-368741e2683e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mirm_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mirm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn_pr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"| pen:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%.3f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mirm_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-192-6f0c8d6ea272>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, model, x, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0minput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mget_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             raise ValueError('An operation has `None` for gradient. '\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   3023\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_tf_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3024\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3025\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients_v2\u001b[0;34m(ys, xs, grad_ys, name, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    275\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0mstop_gradient_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     reachable_to_ops, pending_count, loop_state = _PendingCount(\n\u001b[0;32m--> 551\u001b[0;31m         to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs_set)\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;31m# Iterate over the collected ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_PendingCount\u001b[0;34m(to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs_set)\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;31m# Mark reachable ops from from_ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mreached_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0m_MarkReachedOps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreached_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m   \u001b[0;31m# X in reached_ops iff X is reachable from from_ops by a path of zero or more\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;31m# backpropagatable tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_MarkReachedOps\u001b[0;34m(from_ops, reached_ops, func_graphs)\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_IsBackpropagatable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m           \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Consumers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py\u001b[0m in \u001b[0;36m_Consumers\u001b[0;34m(t, func_graphs)\u001b[0m\n\u001b[1;32m    471\u001b[0m   \u001b[0mconsumers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsumers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_Captures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_t\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mconsumers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Consumers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "step_size = 1000#int(len(e1[1])/10)\n",
    "print(\"Step size:\", step_size)\n",
    "\n",
    "n_pr = 1\n",
    "\n",
    "for epoch in range(5,15):\n",
    "    \n",
    "    print('-'*80)\n",
    "    print('Epoch', epoch)\n",
    "    print('-'*80)\n",
    "    \n",
    "    for batch in range(int(len(e1[1])/step_size)-1):\n",
    "        irm_loss.lambda_ = 0#epoch*10000\n",
    "        if not batch % n_pr: print(\"lam: %6d\" % irm_loss.lambda_, end=' ')\n",
    "        for env in (e1, e2):\n",
    "            x = env[0][batch*step_size:(batch+1)*step_size,:,:,:]\n",
    "            y = env[1][batch*step_size:(batch+1)*step_size]\n",
    "\n",
    "            irm_loss.update(irm_model, x, y)\n",
    "            if not batch % n_pr:\n",
    "                print(\"| pen:\", '%.3f'%irm_loss.penalty, end=' ')\n",
    "                loss, acc = baseline_model.evaluate(env[0], env[1], verbose=0)\n",
    "                print(\"loss:\", '%.3f'%loss, \"acc:\",'%.3f'%acc, end=' ')\n",
    "\n",
    "            baseline_model.fit(x=x, y=y, batch_size=step_size, epochs=1, shuffle=True, verbose=0)\n",
    "\n",
    "        if not batch % n_pr:\n",
    "            loss, acc = baseline_model.evaluate(e3[0],e3[1],verbose=0)\n",
    "            print(\"| e3loss:\", '%.3f'%loss, \"e3acc:\", '%.3f'%acc)\n",
    "\n",
    "for env in (e1, e2, e3):\n",
    "    loss, acc = baseline_model.evaluate(env[0], env[1], verbose=0)\n",
    "    print(\"| e3loss:\", '%.3f'%loss, \"e3acc:\", '%.3f'%acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
