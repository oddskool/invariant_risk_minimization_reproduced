{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Multiply\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv3D, MaxPool3D\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "# convert to RGB\n",
    "x_train = np.stack((x_train,)*3, axis=-1)\n",
    "x_test = np.stack((x_test,)*3, axis=-1)\n",
    "\n",
    "# normalize\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# binary label\n",
    "y_train = (y_train < 5).astype(int)\n",
    "y_test = (y_test < 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(x, y, e, labelflip_proba=.25):\n",
    "    x = x.copy()\n",
    "    y = y.copy()\n",
    "    \n",
    "    y = np.logical_xor(\n",
    "        y,\n",
    "        (np.random.random(size=len(y)) < labelflip_proba).astype(int)\n",
    "    ).astype(int)\n",
    "    \n",
    "    color = np.logical_xor(\n",
    "        y,\n",
    "        (np.random.random(size=len(y)) < e).astype(int)\n",
    "    )\n",
    "    \n",
    "    x[color, :, :, 2] = 0\n",
    "    x[color, :, :, 1] = 0\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_env(x_train, y_train, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, e=.1, batch_size=128, shuffle=True):\n",
    "        \n",
    "        self.e=.1\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.__load_initial_data()\n",
    "        self.__create_envs()\n",
    "        self.__create_validation_envs()\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        print(len(self), 'batches/epoch')\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.e1[1]) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        \n",
    "        batch_indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        e1_x_batch = self.e1[0][batch_indices,:,:,:]\n",
    "        e1_y_batch = self.e1[1][batch_indices]\n",
    "        \n",
    "        return [e1_x_batch, e1_y_batch]\n",
    "    \n",
    "    def __load_initial_data(self):\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        # convert to RGB\n",
    "        x_train = np.stack((x_train,)*3, axis=-1)\n",
    "        x_test = np.stack((x_test,)*3, axis=-1)\n",
    "\n",
    "        # normalize\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "\n",
    "        # binary label\n",
    "        y_train = (y_train < 5).astype(int)\n",
    "        y_test = (y_test < 5).astype(int)\n",
    "        \n",
    "        self.original_data = {\n",
    "            'x_train':x_train,\n",
    "            'x_test':x_test,\n",
    "            'y_train':y_train,\n",
    "            'y_test':y_test\n",
    "        }\n",
    "        \n",
    "    def __create_envs(self):\n",
    "        self.e1 = self.__create_env(self.original_data['x_train'][:10000], \n",
    "                                    self.original_data['y_train'][:10000], .1)\n",
    "        \n",
    "    def __create_validation_envs(self):\n",
    "        self.e11 = self.__create_env(self.original_data['x_train'][30000:40000], \n",
    "                                     self.original_data['y_train'][30000:40000], .1)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indices = np.arange(len(self.e1[1]))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __create_env(self, x, y, e, labelflip_proba=.25):\n",
    "        x = x.copy()\n",
    "        y = y.copy()\n",
    "\n",
    "        y = np.logical_xor(\n",
    "            y,\n",
    "            (np.random.random(size=len(y)) < labelflip_proba).astype(int)\n",
    "        ).astype(int)\n",
    "\n",
    "        color = np.logical_xor(\n",
    "            y,\n",
    "            (np.random.random(size=len(y)) < e).astype(int)\n",
    "        )\n",
    "\n",
    "        x[color, :, :, 2] = 0\n",
    "        x[color, :, :, 1] = 0\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(compile=False):\n",
    "    \n",
    "    input_images = Input(shape=(28, 28, 3))\n",
    "    \n",
    "    cnn = Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu')(input_images)\n",
    "    cnn = Conv2D(64, (3, 3), activation='relu')(cnn)\n",
    "    cnn = MaxPooling2D(pool_size=(2, 2))(cnn)\n",
    "    cnn = Dropout(0.25)(cnn)\n",
    "    cnn = Flatten()(cnn)\n",
    "    \n",
    "    env1 = Dense(32, activation='relu')(cnn)\n",
    "    env1 = Dropout(0.5)(env1)\n",
    "    env1 = Dense(1, name='env1')(env1)\n",
    "        \n",
    "    model = Model(\n",
    "        inputs=[input_images],\n",
    "        outputs=[env1]\n",
    "    )\n",
    "    \n",
    "    if compile:\n",
    "        model.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.binary_crossentropy,\n",
    "            ],\n",
    "            optimizer=tf.keras.optimizers.Adadelta(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 batches/epoch\n"
     ]
    }
   ],
   "source": [
    "g = MNISTDataGenerator()\n",
    "d = tf.data.Dataset.from_tensor_slices(g.e1).shuffle(256).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((), ((None, 28, 28, 3), (None,))), types: (tf.int64, (tf.float32, tf.int64))>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.enumerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, valid_dataset, epochs, \n",
    "          lambda_=1.0, \n",
    "          dummy=tf.convert_to_tensor([1.]),\n",
    "          loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "          accuracy_object = tf.keras.metrics.Accuracy(),\n",
    "          optimizer = tf.keras.optimizers.Adam()):\n",
    "  for epoch in range(epochs):\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "    \n",
    "      # compute penalty\n",
    "      with tf.GradientTape() as tape:\n",
    "        tape.watch(dummy)\n",
    "        logits = model(images, training=False)\n",
    "        loss_value = loss_object(labels, logits * dummy)\n",
    "      accuracy_object.update_state(labels, \n",
    "                                   tf.math.greater(\n",
    "                                       tf.keras.activations.sigmoid(logits),\n",
    "                                       .5)\n",
    "                                   )\n",
    "      grads = tape.gradient(loss_value, dummy)\n",
    "      penalty = tf.math.reduce_mean(loss_value * tf.math.square(grads)).numpy()\n",
    "    \n",
    "      # train\n",
    "      with tf.GradientTape() as tape:\n",
    "        logits = model(images, training=True)\n",
    "        loss_value = loss_object(labels, logits)\n",
    "      grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "      grads += penalty * lambda_\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "      if not batch % 30:\n",
    "        tr_acc = accuracy_object.result().numpy()\n",
    "        accuracy_object.reset_states()\n",
    "        # validation\n",
    "        for (v_batch, (v_images, v_labels)) in enumerate(valid_dataset):\n",
    "          logits = model(v_images, training=False)\n",
    "          accuracy_object.update_state(v_labels, \n",
    "                                       tf.math.greater(\n",
    "                                         tf.keras.activations.sigmoid(logits),\n",
    "                                         .5)\n",
    "                                       )\n",
    "        v_acc = accuracy_object.result().numpy()\n",
    "        accuracy_object.reset_states()\n",
    "        print ('Epoch %3d TrainLoss %.5f Penalty %.5f TrainAcc %.3f TestAcc %.3f' % (\n",
    "            epoch, loss_value.numpy().mean(), penalty, tr_acc, v_acc \n",
    "        ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 TrainLoss 0.69328 Penalty 0.00001 TrainAcc 0.477 TestAcc 0.500\n",
      "Epoch   0 TrainLoss 0.31873 Penalty 0.00228 TrainAcc 0.870 TestAcc 0.896\n",
      "Epoch   0 TrainLoss 0.31029 Penalty 0.00001 TrainAcc 0.900 TestAcc 0.896\n",
      "Epoch   1 TrainLoss 0.27273 Penalty 0.00013 TrainAcc 0.894 TestAcc 0.896\n",
      "Epoch   1 TrainLoss 0.35763 Penalty 0.00037 TrainAcc 0.895 TestAcc 0.896\n",
      "Epoch   1 TrainLoss 0.42485 Penalty 0.00392 TrainAcc 0.901 TestAcc 0.896\n",
      "Epoch   2 TrainLoss 0.18218 Penalty 0.00111 TrainAcc 0.898 TestAcc 0.896\n",
      "Epoch   2 TrainLoss 0.34221 Penalty 0.00001 TrainAcc 0.894 TestAcc 0.896\n",
      "Epoch   2 TrainLoss 0.27341 Penalty 0.00017 TrainAcc 0.901 TestAcc 0.896\n",
      "Epoch   3 TrainLoss 0.29819 Penalty 0.00019 TrainAcc 0.894 TestAcc 0.896\n",
      "Epoch   3 TrainLoss 0.38884 Penalty 0.00023 TrainAcc 0.896 TestAcc 0.895\n",
      "Epoch   3 TrainLoss 0.30577 Penalty 0.00002 TrainAcc 0.899 TestAcc 0.896\n",
      "Epoch   4 TrainLoss 0.32601 Penalty 0.00246 TrainAcc 0.895 TestAcc 0.896\n",
      "Epoch   4 TrainLoss 0.35987 Penalty 0.00010 TrainAcc 0.896 TestAcc 0.896\n",
      "Epoch   4 TrainLoss 0.26930 Penalty 0.00014 TrainAcc 0.902 TestAcc 0.896\n",
      "Epoch   5 TrainLoss 0.15553 Penalty 0.00150 TrainAcc 0.899 TestAcc 0.896\n",
      "Epoch   5 TrainLoss 0.32909 Penalty 0.00000 TrainAcc 0.895 TestAcc 0.896\n",
      "Epoch   5 TrainLoss 0.38358 Penalty 0.00206 TrainAcc 0.900 TestAcc 0.892\n",
      "Epoch   6 TrainLoss 0.27380 Penalty 0.00000 TrainAcc 0.897 TestAcc 0.895\n",
      "Epoch   6 TrainLoss 0.22902 Penalty 0.00293 TrainAcc 0.895 TestAcc 0.895\n",
      "Epoch   6 TrainLoss 0.38852 Penalty 0.00167 TrainAcc 0.900 TestAcc 0.896\n",
      "Epoch   7 TrainLoss 0.21906 Penalty 0.00031 TrainAcc 0.900 TestAcc 0.896\n",
      "Epoch   7 TrainLoss 0.31172 Penalty 0.00035 TrainAcc 0.894 TestAcc 0.895\n",
      "Epoch   7 TrainLoss 0.28050 Penalty 0.00022 TrainAcc 0.901 TestAcc 0.896\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-293-90c8179ef3d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-292-7a09be73fcb8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, valid_dataset, epochs, lambda_, dummy, loss_object, accuracy_object, optimizer)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    594\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    597\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    598\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1242\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    get_model(), \n",
    "    tf.data.Dataset.from_tensor_slices(g.e1).shuffle(256).batch(128), \n",
    "    tf.data.Dataset.from_tensor_slices(g.e11).shuffle(256).batch(128), \n",
    "    epochs = 10, \n",
    "    lambda_=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 TrainLoss 0.68259 Penalty 0.00001 TrainAcc 0.506 TestAcc 0.751\n",
      "Epoch   0 TrainLoss 0.57051 Penalty 0.00122 TrainAcc 0.853 TestAcc 0.887\n",
      "Epoch   0 TrainLoss 0.56480 Penalty 0.00213 TrainAcc 0.888 TestAcc 0.885\n",
      "Epoch   1 TrainLoss 0.45311 Penalty 0.01056 TrainAcc 0.878 TestAcc 0.870\n",
      "Epoch   1 TrainLoss 0.47709 Penalty 0.00943 TrainAcc 0.877 TestAcc 0.865\n",
      "Epoch   1 TrainLoss 0.52203 Penalty 0.00207 TrainAcc 0.868 TestAcc 0.860\n",
      "Epoch   2 TrainLoss 0.40654 Penalty 0.00887 TrainAcc 0.869 TestAcc 0.861\n",
      "Epoch   2 TrainLoss 0.41467 Penalty 0.00582 TrainAcc 0.858 TestAcc 0.855\n",
      "Epoch   2 TrainLoss 0.39508 Penalty 0.00664 TrainAcc 0.867 TestAcc 0.865\n",
      "Epoch   3 TrainLoss 0.38418 Penalty 0.00670 TrainAcc 0.867 TestAcc 0.862\n",
      "Epoch   3 TrainLoss 0.51284 Penalty 0.00073 TrainAcc 0.868 TestAcc 0.863\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    get_model(), \n",
    "    tf.data.Dataset.from_tensor_slices(g.e1).shuffle(256).batch(128), \n",
    "    tf.data.Dataset.from_tensor_slices(g.e11).shuffle(256).batch(128), \n",
    "    epochs = 10, \n",
    "    lambda_=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
