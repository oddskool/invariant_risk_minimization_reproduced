{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Multiply\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv3D, MaxPool3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColoredMNISTEnvironments():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.__load_initial_data()\n",
    "        self.__create_envs()\n",
    "        self.__create_validation_envs()\n",
    "\n",
    "    def __load_initial_data(self):\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        # convert to RGB\n",
    "        x_train = np.stack((x_train,)*3, axis=-1)\n",
    "        x_test = np.stack((x_test,)*3, axis=-1)\n",
    "\n",
    "        # normalize\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "\n",
    "        # binary label\n",
    "        y_train = (y_train < 5).astype(int)\n",
    "        y_test = (y_test < 5).astype(int)\n",
    "        \n",
    "        self.original_data = {\n",
    "            'x_train':x_train,\n",
    "            'x_test':x_test,\n",
    "            'y_train':y_train,\n",
    "            'y_test':y_test\n",
    "        }\n",
    "        \n",
    "    def __create_envs(self):\n",
    "        k=10**4\n",
    "        self.e1 = self.__create_env(self.original_data['x_train'][:k], \n",
    "                                    self.original_data['y_train'][:k], .1)\n",
    "        self.e2 = self.__create_env(self.original_data['x_train'][k:2*k], \n",
    "                                    self.original_data['y_train'][k:2*k], .2)\n",
    "        self.e3 = self.__create_env(self.original_data['x_train'][2*k:3*k], \n",
    "                                    self.original_data['y_train'][2*k:3*k], .9)\n",
    "        \n",
    "    def __create_validation_envs(self):\n",
    "        k=10**4\n",
    "        i=3*k\n",
    "        self.e11 = self.__create_env(self.original_data['x_train'][i:i+k], \n",
    "                                     self.original_data['y_train'][i:i+k], .1)\n",
    "        self.e22 = self.__create_env(self.original_data['x_train'][i+k:i+2*k], \n",
    "                                     self.original_data['y_train'][i+k:i+2*k], .2)\n",
    "        self.e33 = self.__create_env(self.original_data['x_train'][i+2*k:i+3*k], \n",
    "                                     self.original_data['y_train'][i+2*k:i+3*k], .9)\n",
    "        \n",
    "    def __create_env(self, x, y, e, labelflip_proba=.25):\n",
    "        x = x.copy()\n",
    "        y = y.copy()\n",
    "\n",
    "        y = np.logical_xor(\n",
    "            y,\n",
    "            (np.random.random(size=len(y)) < labelflip_proba).astype(int)\n",
    "        ).astype(int)\n",
    "\n",
    "        color = np.logical_xor(\n",
    "            y,\n",
    "            (np.random.random(size=len(y)) < e).astype(int)\n",
    "        )\n",
    "\n",
    "        x[color, :, :, 2] = 0\n",
    "        x[color, :, :, 1] = 0\n",
    "        return tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        \n",
    "    def joint_iterator(self, shuffle_sz=256, batch_sz=128):\n",
    "        def \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_final_units=32, compile=False):\n",
    "    \n",
    "    input_images = Input(shape=(28, 28, 3))\n",
    "    \n",
    "    cnn = Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu')(input_images)\n",
    "    cnn = Conv2D(64, (3, 3), activation='relu')(cnn)\n",
    "    cnn = MaxPooling2D(pool_size=(2, 2))(cnn)\n",
    "    cnn = Dropout(0.25)(cnn)\n",
    "    cnn = Flatten()(cnn)\n",
    "    \n",
    "    env1 = Dense(32, activation='relu')(cnn)\n",
    "    env1 = Dropout(0.5)(env1)\n",
    "    env1 = Dense(1, name='env1')(env1)\n",
    "        \n",
    "    model = Model(\n",
    "        inputs=[input_images],\n",
    "        outputs=[env1]\n",
    "    )\n",
    "    \n",
    "    if compile:\n",
    "        model.compile(\n",
    "            loss=[\n",
    "                tf.keras.losses.binary_crossentropy,\n",
    "            ],\n",
    "            optimizer=tf.keras.optimizers.Adadelta(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "class IRMModel(object):\n",
    "    \n",
    "    def __init__(self, model = get_model(), optimizer = tf.keras.optimizers.Adam()):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.envs = ColoredMNISTEnvironments()\n",
    "        self.dummy = tf.convert_to_tensor([1.])\n",
    "        self.loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.logs = defaultdict(list)\n",
    "        self.logdir = \"/home/e.diemert/tflogs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.file_writer = tf.summary.create_file_writer(self.logdir + \"/metrics\")\n",
    "        self.file_writer.set_as_default()\n",
    "\n",
    "        \n",
    "    def evaluate(self, env):\n",
    "        accuracy = tf.keras.metrics.Accuracy()\n",
    "        loss = tf.keras.metrics.BinaryCrossentropy()\n",
    "        per_batch_penalties = []\n",
    "        for (batch, (x, y)) in enumerate(env):\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(self.dummy)\n",
    "                logits = self.model(x, training=False)\n",
    "                dummy_loss = self.loss(y, logits) * self.dummy\n",
    "            batch_grads = tape.gradient(dummy_loss, [self.dummy])\n",
    "            per_batch_penalties += [\n",
    "                tf.math.square(\n",
    "                    dummy_loss * batch_grads\n",
    "                )\n",
    "            ]\n",
    "            loss.update_state(y, logits)\n",
    "            accuracy.update_state(y, tf.math.greater(tf.keras.activations.sigmoid(logits), .5))\n",
    "        return loss.result().numpy(), accuracy.result().numpy(), tf.reduce_mean(per_batch_penalties)\n",
    "    \n",
    "    def compute_penalty(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.dummy)\n",
    "            dummy_logits = self.model(x, training=True)\n",
    "            dummy_loss = self.loss(y, dummy_logits * self.dummy)\n",
    "        dummy_grads = tape.gradient(dummy_loss, self.dummy)\n",
    "        dummy_penalty = dummy_grads ** 2\n",
    "        return dummy_penalty\n",
    "        \n",
    "    def batch_gradients(self, x, y, penalty):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(penalty)\n",
    "            logits = self.model(x, training=True)\n",
    "            loss_value = self.loss(y, logits) + penalty\n",
    "        grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "        return loss_value, grads\n",
    "    \n",
    "    def do_evaluations(self, epoch, print_=True, batch_size=128):\n",
    "        if print_:\n",
    "            print('-'*80)\n",
    "            print(\"epoch:\", epoch)\n",
    "        ood_loss, ood_acc, ood_penalty = self.evaluate(self.envs.e3.shuffle(2*batch_size).batch(batch_size))\n",
    "        self.logs['ood-loss'] += [ood_loss]\n",
    "        self.logs['ood-acc'] += [ood_acc]\n",
    "        self.logs['ood-penalty'] += [ood_penalty.numpy()]\n",
    "        self.log_event('ood_loss', ood_loss, epoch)\n",
    "        self.log_event('ood_acc', ood_acc, epoch)\n",
    "        self.log_event('ood_pen', ood_penalty.numpy(), epoch)\n",
    "        if print_:\n",
    "            print('ood  loss %.5f acc: %.3f'%(ood_loss, ood_acc))   \n",
    "        for env_name, env in (('e11',self.envs.e11.shuffle(2*batch_size).batch(batch_size)), \n",
    "                              ('e22',self.envs.e22.shuffle(2*batch_size).batch(batch_size)),):\n",
    "            env_loss, env_acc, env_penalty = self.evaluate(env)\n",
    "            self.logs[env_name+'-test-loss'] += [env_loss]\n",
    "            self.logs[env_name+'-test-acc'] += [env_acc]\n",
    "            self.log_event(env_name+'_loss', env_loss, epoch)\n",
    "            self.log_event(env_name+'_acc', env_acc, epoch)\n",
    "            self.log_event(env_name+'_pen', env_penalty.numpy(), epoch)\n",
    "            if print_:\n",
    "                print('%s loss %.5f acc: %.3f'%(env_name, env_loss, env_acc))\n",
    "        if print_:\n",
    "            print('-'*80)\n",
    "\n",
    "    def log_event(self, event, value, epoch):\n",
    "        tf.summary.scalar(event, data=value, step=epoch)\n",
    "                \n",
    "    def train(self, epochs, lambda_, batch_size=128):\n",
    "        for epoch in range(epochs):\n",
    "            d1 = self.envs.e1.shuffle(2*batch_size).batch(batch_size).__iter__()\n",
    "            d2 = self.envs.e2.shuffle(2*batch_size).batch(batch_size).__iter__()\n",
    "            batch = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    x1, y1 = d1.next()\n",
    "                    x2, y2 = d2.next()\n",
    "                    pen1 = self.compute_penalty(x1, y1)\n",
    "                    pen2 = self.compute_penalty(x2, y2)\n",
    "                    self.logs['e1'+'-train-penalty'] += pen1.numpy()\n",
    "                    self.logs['e2'+'-train-penalty'] += pen2.numpy()\n",
    "                    pen = tf.reduce_mean([pen1, pen2])\n",
    "                    l1, grads1 = self.batch_gradients(x1, y1, lambda_(epoch) * pen)\n",
    "                    l2, grads2 = self.batch_gradients(x2, y2, lambda_(epoch) * pen)\n",
    "                    grads = [ tf.reduce_mean([grads1[_], grads2[_]], axis=0) for _ in range(len(grads1)) ]\n",
    "                    self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "                    self.logs['e1'+'-train-loss'] += [l1]\n",
    "                    self.logs['e2'+'-train-loss'] += [l2]\n",
    "                    if not batch % 10:\n",
    "                        print(\"%4d\"%batch, \n",
    "                              \"tr-l1: %.5f\"%l1.numpy(), \"tr-p1: %.5f\"%(pen1.numpy()*lambda_(epoch)), \n",
    "                              \"tr-l2: %.5f\"%l2.numpy(), \"tr-p2: %.5f\"%(pen2.numpy()*lambda_(epoch)))\n",
    "                    batch += 1\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            self.do_evaluations(epoch, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 tr-l1: 0.68825 tr-p1: 0.00000 tr-l2: 0.69205 tr-p2: 0.00000\n",
      "   3 tr-l1: 0.43447 tr-p1: 0.00000 tr-l2: 0.51341 tr-p2: 0.00000\n",
      "   6 tr-l1: 0.47731 tr-p1: 0.00000 tr-l2: 0.52048 tr-p2: 0.00000\n",
      "   9 tr-l1: 0.30967 tr-p1: 0.00000 tr-l2: 0.54819 tr-p2: 0.00000\n",
      "  12 tr-l1: 0.34170 tr-p1: 0.00000 tr-l2: 0.46500 tr-p2: 0.00000\n",
      "  15 tr-l1: 0.35993 tr-p1: 0.00000 tr-l2: 0.52421 tr-p2: 0.00000\n",
      "  18 tr-l1: 0.39303 tr-p1: 0.00000 tr-l2: 0.43595 tr-p2: 0.00000\n",
      "  21 tr-l1: 0.33504 tr-p1: 0.00000 tr-l2: 0.54854 tr-p2: 0.00000\n",
      "  24 tr-l1: 0.36328 tr-p1: 0.00000 tr-l2: 0.49512 tr-p2: 0.00000\n",
      "  27 tr-l1: 0.38166 tr-p1: 0.00000 tr-l2: 0.53743 tr-p2: 0.00000\n",
      "  30 tr-l1: 0.33367 tr-p1: 0.00000 tr-l2: 0.47294 tr-p2: 0.00000\n",
      "  33 tr-l1: 0.39552 tr-p1: 0.00000 tr-l2: 0.54928 tr-p2: 0.00000\n",
      "  36 tr-l1: 0.29414 tr-p1: 0.00000 tr-l2: 0.46333 tr-p2: 0.00000\n",
      "  39 tr-l1: 0.18042 tr-p1: 0.00000 tr-l2: 0.39414 tr-p2: 0.00000\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 0\n",
      "ood  loss 13.68001 acc: 0.107\n",
      "e11 loss 1.59122 acc: 0.895\n",
      "e22 loss 3.22934 acc: 0.791\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 0.32983 tr-p1: 0.00000 tr-l2: 0.45909 tr-p2: 0.00000\n",
      "   3 tr-l1: 0.36379 tr-p1: 0.00000 tr-l2: 0.62053 tr-p2: 0.00000\n",
      "   6 tr-l1: 0.31510 tr-p1: 0.00000 tr-l2: 0.49604 tr-p2: 0.00000\n",
      "   9 tr-l1: 0.35247 tr-p1: 0.00000 tr-l2: 0.52718 tr-p2: 0.00000\n",
      "  12 tr-l1: 0.35498 tr-p1: 0.00000 tr-l2: 0.49554 tr-p2: 0.00000\n",
      "  15 tr-l1: 0.34092 tr-p1: 0.00000 tr-l2: 0.39404 tr-p2: 0.00000\n",
      "  18 tr-l1: 0.28848 tr-p1: 0.00000 tr-l2: 0.43064 tr-p2: 0.00000\n",
      "  21 tr-l1: 0.30954 tr-p1: 0.00000 tr-l2: 0.46842 tr-p2: 0.00000\n",
      "  24 tr-l1: 0.33837 tr-p1: 0.00000 tr-l2: 0.53868 tr-p2: 0.00000\n",
      "  27 tr-l1: 0.29942 tr-p1: 0.00000 tr-l2: 0.48871 tr-p2: 0.00000\n",
      "  30 tr-l1: 0.32829 tr-p1: 0.00000 tr-l2: 0.48655 tr-p2: 0.00000\n",
      "  33 tr-l1: 0.34399 tr-p1: 0.00000 tr-l2: 0.50292 tr-p2: 0.00000\n",
      "  36 tr-l1: 0.33058 tr-p1: 0.00000 tr-l2: 0.46395 tr-p2: 0.00000\n",
      "  39 tr-l1: 0.21652 tr-p1: 0.00000 tr-l2: 0.22819 tr-p2: 0.00000\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 1\n",
      "ood  loss 12.44604 acc: 0.107\n",
      "e11 loss 1.42591 acc: 0.896\n",
      "e22 loss 2.89308 acc: 0.791\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 34.10960 tr-p1: 57.42801 tr-l2: 34.19271 tr-p2: 10.17737\n",
      "   3 tr-l1: 202.14543 tr-p1: 0.20078 tr-l2: 202.34238 tr-p2: 403.47000\n",
      "   6 tr-l1: 46.27261 tr-p1: 19.53011 tr-l2: 46.41009 tr-p2: 72.34625\n",
      "   9 tr-l1: 100.57759 tr-p1: 156.29329 tr-l2: 100.79446 tr-p2: 44.23065\n",
      "  12 tr-l1: 59.78386 tr-p1: 85.76655 tr-l2: 59.96262 tr-p2: 33.09739\n",
      "  15 tr-l1: 70.08737 tr-p1: 136.09529 tr-l2: 70.22071 tr-p2: 3.43685\n",
      "  18 tr-l1: 30.37687 tr-p1: 12.41132 tr-l2: 30.50548 tr-p2: 47.69231\n",
      "  21 tr-l1: 155.78360 tr-p1: 39.16985 tr-l2: 155.99719 tr-p2: 271.82626\n",
      "  24 tr-l1: 123.66218 tr-p1: 44.83734 tr-l2: 123.91271 tr-p2: 201.91150\n",
      "  27 tr-l1: 106.51661 tr-p1: 150.68228 tr-l2: 106.67531 tr-p2: 61.70083\n",
      "  30 tr-l1: 47.92609 tr-p1: 94.73182 tr-l2: 47.98750 tr-p2: 0.37488\n",
      "  33 tr-l1: 50.43024 tr-p1: 95.19766 tr-l2: 50.52991 tr-p2: 5.01412\n",
      "  36 tr-l1: 22.94135 tr-p1: 15.33917 tr-l2: 23.05274 tr-p2: 29.93113\n",
      "  39 tr-l1: 171.31374 tr-p1: 341.69119 tr-l2: 171.68135 tr-p2: 0.68534\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 2\n",
      "ood  loss 12.27249 acc: 0.108\n",
      "e11 loss 1.43335 acc: 0.895\n",
      "e22 loss 2.92161 acc: 0.791\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 27.97550 tr-p1: 55.13852 tr-l2: 28.00838 tr-p2: 0.21590\n",
      "   3 tr-l1: 70.04148 tr-p1: 7.95094 tr-l2: 70.18901 tr-p2: 131.56609\n",
      "   6 tr-l1: 137.45856 tr-p1: 49.48086 tr-l2: 137.63481 tr-p2: 224.78424\n",
      "   9 tr-l1: 129.18085 tr-p1: 49.49277 tr-l2: 129.37985 tr-p2: 208.19579\n",
      "  12 tr-l1: 39.83700 tr-p1: 69.35662 tr-l2: 39.87916 tr-p2: 9.62731\n",
      "  15 tr-l1: 90.60498 tr-p1: 165.04366 tr-l2: 90.80897 tr-p2: 15.62424\n",
      "  18 tr-l1: 27.26833 tr-p1: 53.63711 tr-l2: 27.38657 tr-p2: 0.30768\n",
      "  21 tr-l1: 74.91338 tr-p1: 50.22831 tr-l2: 75.04700 tr-p2: 98.95133\n",
      "  24 tr-l1: 181.34389 tr-p1: 21.54528 tr-l2: 181.59410 tr-p2: 340.51483\n",
      "  27 tr-l1: 24.20622 tr-p1: 23.25184 tr-l2: 24.34168 tr-p2: 24.50546\n",
      "  30 tr-l1: 25.43549 tr-p1: 50.14038 tr-l2: 25.52382 tr-p2: 0.00038\n",
      "  33 tr-l1: 25.18543 tr-p1: 49.33365 tr-l2: 25.24431 tr-p2: 0.31377\n",
      "  36 tr-l1: 33.63975 tr-p1: 56.04141 tr-l2: 33.76195 tr-p2: 10.65121\n",
      "  39 tr-l1: 475.70337 tr-p1: 36.47747 tr-l2: 476.22974 tr-p2: 914.55579\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 3\n",
      "ood  loss 12.09397 acc: 0.108\n",
      "e11 loss 1.41696 acc: 0.895\n",
      "e22 loss 2.87243 acc: 0.791\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 23.07064 tr-p1: 0.43067 tr-l2: 23.14901 tr-p2: 45.05476\n",
      "   3 tr-l1: 112.02954 tr-p1: 6.52499 tr-l2: 112.17532 tr-p2: 216.86226\n",
      "   6 tr-l1: 52.61969 tr-p1: 27.68192 tr-l2: 52.78895 tr-p2: 76.85104\n",
      "   9 tr-l1: 58.86105 tr-p1: 70.81786 tr-l2: 58.94042 tr-p2: 46.15033\n",
      "  12 tr-l1: 120.81538 tr-p1: 160.60413 tr-l2: 121.00765 tr-p2: 80.44163\n",
      "  15 tr-l1: 19.95921 tr-p1: 20.92405 tr-l2: 20.05865 tr-p2: 18.27782\n",
      "  18 tr-l1: 72.35797 tr-p1: 0.23479 tr-l2: 72.51968 tr-p2: 143.74837\n",
      "  21 tr-l1: 67.93590 tr-p1: 134.09401 tr-l2: 68.05257 tr-p2: 1.17726\n",
      "  24 tr-l1: 78.30183 tr-p1: 47.26468 tr-l2: 78.48960 tr-p2: 108.69975\n",
      "  27 tr-l1: 151.26047 tr-p1: 32.02156 tr-l2: 151.50737 tr-p2: 269.85666\n",
      "  30 tr-l1: 74.22268 tr-p1: 121.36940 tr-l2: 74.41144 tr-p2: 26.46962\n",
      "  33 tr-l1: 35.69582 tr-p1: 64.24876 tr-l2: 35.76530 tr-p2: 6.42864\n",
      "  36 tr-l1: 82.56274 tr-p1: 50.18316 tr-l2: 82.74496 tr-p2: 114.32158\n",
      "  39 tr-l1: 167.05336 tr-p1: 310.84875 tr-l2: 167.32861 tr-p2: 23.10965\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 4\n",
      "ood  loss 11.42358 acc: 0.107\n",
      "e11 loss 1.33570 acc: 0.896\n",
      "e22 loss 2.71516 acc: 0.791\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 19.03520 tr-p1: 4.52367 tr-l2: 19.09322 tr-p2: 32.95637\n",
      "   3 tr-l1: 2219.97388 tr-p1: 56.77526 tr-l2: 2220.16602 tr-p2: 4382.52985\n",
      "   6 tr-l1: 151.22420 tr-p1: 121.52399 tr-l2: 151.28310 tr-p2: 180.21299\n",
      "   9 tr-l1: 1192.99988 tr-p1: 1205.84611 tr-l2: 1193.21106 tr-p2: 1179.54919\n",
      "  12 tr-l1: 579.11835 tr-p1: 1157.57212 tr-l2: 579.22675 tr-p2: 0.01692\n",
      "  15 tr-l1: 167.53215 tr-p1: 278.39553 tr-l2: 167.64099 tr-p2: 55.97352\n",
      "  18 tr-l1: 468.06683 tr-p1: 831.56656 tr-l2: 468.22150 tr-p2: 104.00122\n",
      "  21 tr-l1: 198.40143 tr-p1: 185.05328 tr-l2: 198.55196 tr-p2: 211.17195\n",
      "  24 tr-l1: 2321.64648 tr-p1: 533.99247 tr-l2: 2321.90503 tr-p2: 4108.69122\n",
      "  27 tr-l1: 697.95874 tr-p1: 712.74755 tr-l2: 698.14746 tr-p2: 682.61349\n",
      "  30 tr-l1: 904.46405 tr-p1: 845.59834 tr-l2: 904.64966 tr-p2: 962.71494\n",
      "  33 tr-l1: 371.45654 tr-p1: 697.72773 tr-l2: 371.57394 tr-p2: 44.56833\n",
      "  36 tr-l1: 445.76706 tr-p1: 454.03009 tr-l2: 445.92731 tr-p2: 436.89334\n",
      "  39 tr-l1: 788.17969 tr-p1: 1130.33140 tr-l2: 788.26044 tr-p2: 445.57941\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 5\n",
      "ood  loss 11.51922 acc: 0.116\n",
      "e11 loss 1.39396 acc: 0.894\n",
      "e22 loss 2.71900 acc: 0.791\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 159.24797 tr-p1: 268.78486 tr-l2: 159.32684 tr-p2: 49.12185\n",
      "   3 tr-l1: 453.09851 tr-p1: 325.34178 tr-l2: 453.11972 tr-p2: 580.10216\n",
      "   6 tr-l1: 981.52161 tr-p1: 261.61466 tr-l2: 981.73096 tr-p2: 1700.79842\n",
      "   9 tr-l1: 551.90448 tr-p1: 967.62609 tr-l2: 552.07281 tr-p2: 135.59166\n",
      "  12 tr-l1: 422.91022 tr-p1: 822.11848 tr-l2: 423.05655 tr-p2: 23.10342\n",
      "  15 tr-l1: 689.17926 tr-p1: 1357.02724 tr-l2: 689.34821 tr-p2: 20.74371\n",
      "  18 tr-l1: 747.93396 tr-p1: 0.50349 tr-l2: 748.06396 tr-p2: 1494.60007\n",
      "  21 tr-l1: 1208.40393 tr-p1: 728.22864 tr-l2: 1208.54150 tr-p2: 1688.02459\n",
      "  24 tr-l1: 1694.80713 tr-p1: 416.29653 tr-l2: 1695.05530 tr-p2: 2972.72932\n",
      "  27 tr-l1: 991.31012 tr-p1: 1165.96827 tr-l2: 991.51666 tr-p2: 816.07616\n",
      "  30 tr-l1: 951.81561 tr-p1: 1862.86699 tr-l2: 952.00220 tr-p2: 40.19272\n",
      "  33 tr-l1: 1369.33936 tr-p1: 2577.24747 tr-l2: 1369.52209 tr-p2: 160.85709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  36 tr-l1: 173.41801 tr-p1: 292.35021 tr-l2: 173.49834 tr-p2: 53.86415\n",
      "  39 tr-l1: 1619.60046 tr-p1: 3196.16571 tr-l2: 1619.71313 tr-p2: 42.73580\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 6\n",
      "ood  loss 11.50330 acc: 0.114\n",
      "e11 loss 1.35769 acc: 0.894\n",
      "e22 loss 2.71768 acc: 0.791\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 606.13275 tr-p1: 789.53719 tr-l2: 606.34174 tr-p2: 422.31246\n",
      "   3 tr-l1: 1072.59705 tr-p1: 94.71638 tr-l2: 1072.70813 tr-p2: 2049.82460\n",
      "   6 tr-l1: 528.60510 tr-p1: 712.35346 tr-l2: 528.78760 tr-p2: 344.35086\n",
      "   9 tr-l1: 452.87686 tr-p1: 717.43303 tr-l2: 452.99728 tr-p2: 187.62981\n",
      "  12 tr-l1: 304.70078 tr-p1: 413.93936 tr-l2: 304.83075 tr-p2: 194.75438\n",
      "  15 tr-l1: 438.26999 tr-p1: 874.22030 tr-l2: 438.37668 tr-p2: 1.67936\n",
      "  18 tr-l1: 739.67218 tr-p1: 268.68649 tr-l2: 739.81818 tr-p2: 1210.05280\n",
      "  21 tr-l1: 951.71881 tr-p1: 313.29759 tr-l2: 951.95477 tr-p2: 1589.63073\n",
      "  24 tr-l1: 1934.03357 tr-p1: 54.88649 tr-l2: 1934.27075 tr-p2: 3812.49860\n",
      "  27 tr-l1: 698.11292 tr-p1: 1060.78610 tr-l2: 698.29266 tr-p2: 334.88278\n",
      "  30 tr-l1: 269.63895 tr-p1: 514.21262 tr-l2: 269.75604 tr-p2: 24.39495\n",
      "  33 tr-l1: 850.21234 tr-p1: 1413.83726 tr-l2: 850.30621 tr-p2: 285.94094\n",
      "  36 tr-l1: 248.27174 tr-p1: 495.12619 tr-l2: 248.30310 tr-p2: 0.77205\n",
      "  39 tr-l1: 3786.11548 tr-p1: 7313.97346 tr-l2: 3786.11157 tr-p2: 257.45698\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 7\n",
      "ood  loss 11.00534 acc: 0.113\n",
      "e11 loss 1.32477 acc: 0.895\n",
      "e22 loss 2.63156 acc: 0.791\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 58.12108 tr-p1: 9.09765 tr-l2: 58.18808 tr-p2: 106.51276\n",
      "   3 tr-l1: 284.40564 tr-p1: 33.86037 tr-l2: 284.50528 tr-p2: 534.34554\n",
      "   6 tr-l1: 2040.10425 tr-p1: 192.33921 tr-l2: 2040.35608 tr-p2: 3887.27486\n",
      "   9 tr-l1: 1009.19318 tr-p1: 1358.81379 tr-l2: 1009.40100 tr-p2: 658.99985\n",
      "  12 tr-l1: 361.87750 tr-p1: 708.61238 tr-l2: 362.00940 tr-p2: 14.48677\n",
      "  15 tr-l1: 792.69006 tr-p1: 1565.92242 tr-l2: 792.85181 tr-p2: 18.85159\n",
      "  18 tr-l1: 696.60974 tr-p1: 1335.52831 tr-l2: 696.75677 tr-p2: 57.18300\n",
      "  21 tr-l1: 1162.83984 tr-p1: 1209.21265 tr-l2: 1163.08728 tr-p2: 1116.01409\n",
      "  24 tr-l1: 1771.72449 tr-p1: 24.24450 tr-l2: 1771.89062 tr-p2: 3518.52141\n",
      "  27 tr-l1: 603.38281 tr-p1: 42.21156 tr-l2: 603.56091 tr-p2: 1163.87820\n",
      "  30 tr-l1: 607.37207 tr-p1: 1179.94966 tr-l2: 607.47980 tr-p2: 34.10341\n",
      "  33 tr-l1: 721.57953 tr-p1: 1222.06882 tr-l2: 721.67633 tr-p2: 220.44303\n",
      "  36 tr-l1: 260.55453 tr-p1: 414.76884 tr-l2: 260.54956 tr-p2: 105.62460\n",
      "  39 tr-l1: 1610.13501 tr-p1: 2755.03434 tr-l2: 1610.08618 tr-p2: 464.56135\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 8\n",
      "ood  loss 10.78817 acc: 0.116\n",
      "e11 loss 1.32646 acc: 0.895\n",
      "e22 loss 2.56688 acc: 0.791\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 287.26962 tr-p1: 513.87353 tr-l2: 287.35806 tr-p2: 60.09890\n",
      "   3 tr-l1: 1421.51953 tr-p1: 28.71882 tr-l2: 1421.68555 tr-p2: 2813.71437\n",
      "   6 tr-l1: 922.14270 tr-p1: 6.61761 tr-l2: 922.32318 tr-p2: 1837.05073\n",
      "   9 tr-l1: 1864.84937 tr-p1: 808.12927 tr-l2: 1865.14807 tr-p2: 2920.98764\n",
      "  12 tr-l1: 873.35931 tr-p1: 1510.38039 tr-l2: 873.54608 tr-p2: 235.72438\n",
      "  15 tr-l1: 771.80670 tr-p1: 952.62239 tr-l2: 771.86407 tr-p2: 590.32952\n",
      "  18 tr-l1: 761.96381 tr-p1: 1070.76289 tr-l2: 762.16364 tr-p2: 452.65886\n",
      "  21 tr-l1: 1555.53821 tr-p1: 397.22282 tr-l2: 1555.80457 tr-p2: 2713.28110\n",
      "  24 tr-l1: 568.28973 tr-p1: 0.01036 tr-l2: 568.45874 tr-p2: 1135.98313\n",
      "  27 tr-l1: 464.02695 tr-p1: 333.64999 tr-l2: 464.17844 tr-p2: 593.85123\n",
      "  30 tr-l1: 221.33745 tr-p1: 340.64343 tr-l2: 221.44510 tr-p2: 101.37991\n",
      "  33 tr-l1: 887.16669 tr-p1: 1490.86341 tr-l2: 887.24561 tr-p2: 282.89605\n",
      "  36 tr-l1: 660.87567 tr-p1: 1121.81995 tr-l2: 660.95190 tr-p2: 199.31404\n",
      "  39 tr-l1: 1267.12976 tr-p1: 782.82049 tr-l2: 1267.02307 tr-p2: 1750.49268\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 9\n",
      "ood  loss 10.21882 acc: 0.123\n",
      "e11 loss 1.24830 acc: 0.892\n",
      "e22 loss 2.47188 acc: 0.789\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 292.64575 tr-p1: 583.74549 tr-l2: 292.77719 tr-p2: 1.02213\n",
      "   3 tr-l1: 88.92704 tr-p1: 55.78746 tr-l2: 88.99596 tr-p2: 121.45132\n",
      "   6 tr-l1: 2011.15906 tr-p1: 442.40034 tr-l2: 2011.41785 tr-p2: 3579.36695\n",
      "   9 tr-l1: 408.24310 tr-p1: 116.86870 tr-l2: 408.35300 tr-p2: 698.88318\n",
      "  12 tr-l1: 496.08054 tr-p1: 802.14581 tr-l2: 496.28467 tr-p2: 189.45647\n",
      "  15 tr-l1: 148.73257 tr-p1: 280.77313 tr-l2: 148.84157 tr-p2: 16.02230\n",
      "  18 tr-l1: 783.18286 tr-p1: 848.16683 tr-l2: 783.39606 tr-p2: 717.63159\n",
      "  21 tr-l1: 368.80356 tr-p1: 728.74269 tr-l2: 368.89810 tr-p2: 8.29413\n",
      "  24 tr-l1: 385.58005 tr-p1: 351.99977 tr-l2: 385.68021 tr-p2: 418.57189\n",
      "  27 tr-l1: 261.69049 tr-p1: 118.59401 tr-l2: 261.81409 tr-p2: 404.18496\n",
      "  30 tr-l1: 419.83209 tr-p1: 777.79763 tr-l2: 419.93381 tr-p2: 61.31598\n",
      "  33 tr-l1: 575.09650 tr-p1: 1129.47635 tr-l2: 575.20007 tr-p2: 20.10534\n",
      "  36 tr-l1: 411.86566 tr-p1: 515.91159 tr-l2: 411.98087 tr-p2: 307.21053\n",
      "  39 tr-l1: 6783.12402 tr-p1: 1597.84574 tr-l2: 6783.24609 tr-p2: 11967.55916\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 10\n",
      "ood  loss 10.44634 acc: 0.120\n",
      "e11 loss 1.27778 acc: 0.894\n",
      "e22 loss 2.51101 acc: 0.790\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 328.81113 tr-p1: 559.32933 tr-l2: 328.92746 tr-p2: 97.73888\n",
      "   3 tr-l1: 363.25827 tr-p1: 608.99677 tr-l2: 363.40802 tr-p2: 116.98131\n",
      "   6 tr-l1: 444.66776 tr-p1: 113.90472 tr-l2: 444.77115 tr-p2: 774.76799\n",
      "   9 tr-l1: 1012.93652 tr-p1: 1175.79661 tr-l2: 1013.14447 tr-p2: 849.52293\n",
      "  12 tr-l1: 546.75983 tr-p1: 1042.32328 tr-l2: 546.91528 tr-p2: 50.60474\n",
      "  15 tr-l1: 729.24603 tr-p1: 1128.08729 tr-l2: 729.31348 tr-p2: 329.79832\n",
      "  18 tr-l1: 385.53326 tr-p1: 570.39796 tr-l2: 385.64490 tr-p2: 199.98108\n",
      "  21 tr-l1: 522.90607 tr-p1: 437.58592 tr-l2: 523.10620 tr-p2: 607.70842\n",
      "  24 tr-l1: 1475.23242 tr-p1: 382.38445 tr-l2: 1475.49829 tr-p2: 2567.57773\n",
      "  27 tr-l1: 245.22585 tr-p1: 78.78673 tr-l2: 245.37837 tr-p2: 410.99805\n",
      "  30 tr-l1: 264.15860 tr-p1: 522.55453 tr-l2: 264.23474 tr-p2: 5.13397\n",
      "  33 tr-l1: 617.28815 tr-p1: 1100.70324 tr-l2: 617.37354 tr-p2: 133.23068\n",
      "  36 tr-l1: 502.61801 tr-p1: 879.38495 tr-l2: 502.68689 tr-p2: 125.28704\n",
      "  39 tr-l1: 1326.45178 tr-p1: 17.22384 tr-l2: 1326.75146 tr-p2: 2635.05839\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 11\n",
      "ood  loss 10.38494 acc: 0.119\n",
      "e11 loss 1.26400 acc: 0.893\n",
      "e22 loss 2.50348 acc: 0.789\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 538.89960 tr-p1: 1069.88540 tr-l2: 539.04083 tr-p2: 7.41349\n",
      "   3 tr-l1: 131.83128 tr-p1: 35.89001 tr-l2: 131.91907 tr-p2: 227.15051\n",
      "   6 tr-l1: 183.56340 tr-p1: 112.80181 tr-l2: 183.70499 tr-p2: 253.69332\n",
      "   9 tr-l1: 1061.07153 tr-p1: 10.13626 tr-l2: 1061.24731 tr-p2: 2111.32430\n",
      "  12 tr-l1: 456.47522 tr-p1: 238.24875 tr-l2: 456.62820 tr-p2: 674.10381\n",
      "  15 tr-l1: 300.95306 tr-p1: 490.69128 tr-l2: 301.07202 tr-p2: 110.56566\n",
      "  18 tr-l1: 383.75729 tr-p1: 762.66965 tr-l2: 383.85730 tr-p2: 4.22684\n",
      "  21 tr-l1: 455.91879 tr-p1: 613.00010 tr-l2: 456.07693 tr-p2: 298.24476\n",
      "  24 tr-l1: 747.60809 tr-p1: 922.05359 tr-l2: 747.78619 tr-p2: 572.61130\n",
      "  27 tr-l1: 434.32318 tr-p1: 274.06882 tr-l2: 434.47983 tr-p2: 594.01463\n",
      "  30 tr-l1: 483.49277 tr-p1: 548.23910 tr-l2: 483.60199 tr-p2: 418.11196\n",
      "  33 tr-l1: 65.17686 tr-p1: 23.47064 tr-l2: 65.16683 tr-p2: 106.05635\n",
      "  36 tr-l1: 642.02301 tr-p1: 1270.91641 tr-l2: 642.17926 tr-p2: 12.59827\n",
      "  39 tr-l1: 1661.32629 tr-p1: 625.75452 tr-l2: 1661.44556 tr-p2: 2695.88772\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 12\n",
      "ood  loss 9.49105 acc: 0.135\n",
      "e11 loss 1.33530 acc: 0.889\n",
      "e22 loss 2.46260 acc: 0.788\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 225.11749 tr-p1: 405.52593 tr-l2: 225.19144 tr-p2: 44.10770\n",
      "   3 tr-l1: 343.13608 tr-p1: 146.97536 tr-l2: 343.26721 tr-p2: 538.63446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6 tr-l1: 628.97571 tr-p1: 787.58858 tr-l2: 629.16901 tr-p2: 469.86928\n",
      "   9 tr-l1: 455.14041 tr-p1: 17.63229 tr-l2: 455.24326 tr-p2: 891.93527\n",
      "  12 tr-l1: 286.90491 tr-p1: 148.98566 tr-l2: 287.06476 tr-p2: 424.16952\n",
      "  15 tr-l1: 423.03836 tr-p1: 540.88244 tr-l2: 423.14734 tr-p2: 304.56006\n",
      "  18 tr-l1: 311.90085 tr-p1: 516.00537 tr-l2: 312.02698 tr-p2: 107.22094\n",
      "  21 tr-l1: 92.80791 tr-p1: 24.04306 tr-l2: 92.93174 tr-p2: 160.94819\n",
      "  24 tr-l1: 69.18248 tr-p1: 83.72321 tr-l2: 69.28125 tr-p2: 54.03581\n",
      "  27 tr-l1: 410.28043 tr-p1: 616.57415 tr-l2: 410.44238 tr-p2: 203.39701\n",
      "  30 tr-l1: 860.18689 tr-p1: 27.28302 tr-l2: 860.39526 tr-p2: 1692.38765\n",
      "  33 tr-l1: 132.84303 tr-p1: 264.16534 tr-l2: 132.92201 tr-p2: 0.86572\n",
      "  36 tr-l1: 632.34473 tr-p1: 1153.93661 tr-l2: 632.45691 tr-p2: 110.23809\n",
      "  39 tr-l1: 895.75134 tr-p1: 716.81915 tr-l2: 895.64392 tr-p2: 1074.01237\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 13\n",
      "ood  loss 10.40044 acc: 0.119\n",
      "e11 loss 1.28815 acc: 0.893\n",
      "e22 loss 2.58730 acc: 0.790\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 581.53357 tr-p1: 826.54478 tr-l2: 581.60883 tr-p2: 336.06037\n",
      "   3 tr-l1: 2354.94775 tr-p1: 12.69135 tr-l2: 2355.19336 tr-p2: 4696.68023\n",
      "   6 tr-l1: 417.89618 tr-p1: 52.25800 tr-l2: 418.00595 tr-p2: 782.85644\n",
      "   9 tr-l1: 36.44824 tr-p1: 0.36550 tr-l2: 36.51759 tr-p2: 71.88697\n",
      "  12 tr-l1: 298.08484 tr-p1: 229.52768 tr-l2: 298.21271 tr-p2: 365.93233\n",
      "  15 tr-l1: 398.74774 tr-p1: 681.40063 tr-l2: 398.83356 tr-p2: 115.42435\n",
      "  18 tr-l1: 328.81461 tr-p1: 550.28126 tr-l2: 328.89450 tr-p2: 106.68661\n",
      "  21 tr-l1: 318.36847 tr-p1: 587.97523 tr-l2: 318.45020 tr-p2: 48.15896\n",
      "  24 tr-l1: 1067.98840 tr-p1: 750.02811 tr-l2: 1068.19617 tr-p2: 1385.46294\n",
      "  27 tr-l1: 764.20355 tr-p1: 282.34259 tr-l2: 764.43683 tr-p2: 1245.55286\n",
      "  30 tr-l1: 251.22807 tr-p1: 464.16596 tr-l2: 251.37482 tr-p2: 37.73864\n",
      "  33 tr-l1: 138.60306 tr-p1: 214.10100 tr-l2: 138.71181 tr-p2: 62.45074\n",
      "  36 tr-l1: 294.23590 tr-p1: 522.34386 tr-l2: 294.33380 tr-p2: 65.47575\n",
      "  39 tr-l1: 1024.48364 tr-p1: 582.64164 tr-l2: 1024.54749 tr-p2: 1465.83952\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 14\n",
      "ood  loss 9.70835 acc: 0.131\n",
      "e11 loss 1.27538 acc: 0.892\n",
      "e22 loss 2.44051 acc: 0.789\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 130.44327 tr-p1: 211.17204 tr-l2: 130.51373 tr-p2: 49.10452\n",
      "   3 tr-l1: 1011.74152 tr-p1: 265.08041 tr-l2: 1011.92798 tr-p2: 1757.84957\n",
      "   6 tr-l1: 125.79716 tr-p1: 0.25162 tr-l2: 125.91175 tr-p2: 250.76002\n",
      "   9 tr-l1: 2398.63501 tr-p1: 97.89369 tr-l2: 2398.84326 tr-p2: 4698.75298\n",
      "  12 tr-l1: 998.65668 tr-p1: 180.96613 tr-l2: 998.85699 tr-p2: 1815.74691\n",
      "  15 tr-l1: 269.51904 tr-p1: 484.07642 tr-l2: 269.55215 tr-p2: 54.28063\n",
      "  18 tr-l1: 479.64496 tr-p1: 943.19591 tr-l2: 479.78033 tr-p2: 15.54779\n",
      "  21 tr-l1: 697.48096 tr-p1: 1281.94932 tr-l2: 697.66620 tr-p2: 112.54736\n",
      "  24 tr-l1: 1128.53821 tr-p1: 325.13901 tr-l2: 1128.74744 tr-p2: 1931.31827\n",
      "  27 tr-l1: 230.37450 tr-p1: 170.47032 tr-l2: 230.50014 tr-p2: 289.68509\n",
      "  30 tr-l1: 214.95377 tr-p1: 246.11894 tr-l2: 215.06282 tr-p2: 183.07000\n",
      "  33 tr-l1: 112.32181 tr-p1: 130.78325 tr-l2: 112.36437 tr-p2: 93.18346\n",
      "  36 tr-l1: 586.33368 tr-p1: 1096.37612 tr-l2: 586.44049 tr-p2: 75.71766\n",
      "  39 tr-l1: 562.08142 tr-p1: 1072.38051 tr-l2: 562.25989 tr-p2: 51.36994\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 15\n",
      "ood  loss 9.89500 acc: 0.129\n",
      "e11 loss 1.29791 acc: 0.892\n",
      "e22 loss 2.42889 acc: 0.789\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 279.57407 tr-p1: 483.63819 tr-l2: 279.66711 tr-p2: 74.92002\n",
      "   3 tr-l1: 141.43568 tr-p1: 156.62528 tr-l2: 141.53680 tr-p2: 125.68007\n",
      "   6 tr-l1: 1640.62329 tr-p1: 131.35547 tr-l2: 1640.79883 tr-p2: 3149.20470\n",
      "   9 tr-l1: 1159.33325 tr-p1: 243.44134 tr-l2: 1159.49756 tr-p2: 2074.67619\n",
      "  12 tr-l1: 91.88078 tr-p1: 39.39524 tr-l2: 91.99194 tr-p2: 143.62903\n",
      "  15 tr-l1: 959.62793 tr-p1: 1846.14733 tr-l2: 959.73413 tr-p2: 72.52358\n",
      "  18 tr-l1: 767.53650 tr-p1: 1435.32753 tr-l2: 767.62213 tr-p2: 99.19604\n",
      "  21 tr-l1: 620.34094 tr-p1: 917.44205 tr-l2: 620.49530 tr-p2: 322.65524\n",
      "  24 tr-l1: 968.43506 tr-p1: 275.91190 tr-l2: 968.61182 tr-p2: 1660.39187\n",
      "  27 tr-l1: 301.54825 tr-p1: 118.39902 tr-l2: 301.65012 tr-p2: 484.09123\n",
      "  30 tr-l1: 354.58298 tr-p1: 317.09203 tr-l2: 354.73538 tr-p2: 391.46310\n",
      "  33 tr-l1: 921.09406 tr-p1: 1506.45357 tr-l2: 921.18048 tr-p2: 335.20567\n",
      "  36 tr-l1: 564.29297 tr-p1: 970.65084 tr-l2: 564.37415 tr-p2: 157.39059\n",
      "  39 tr-l1: 132.14366 tr-p1: 140.86573 tr-l2: 132.22923 tr-p2: 122.81623\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 16\n",
      "ood  loss 9.63847 acc: 0.144\n",
      "e11 loss 1.37981 acc: 0.888\n",
      "e22 loss 2.56343 acc: 0.788\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 919.57452 tr-p1: 1284.09872 tr-l2: 919.62695 tr-p2: 554.52324\n",
      "   3 tr-l1: 266.28467 tr-p1: 88.56431 tr-l2: 266.40738 tr-p2: 443.44510\n",
      "   6 tr-l1: 106.05959 tr-p1: 167.82291 tr-l2: 106.09374 tr-p2: 43.63802\n",
      "   9 tr-l1: 662.17389 tr-p1: 229.89186 tr-l2: 662.31293 tr-p2: 1093.80297\n",
      "  12 tr-l1: 765.91895 tr-p1: 1289.57126 tr-l2: 766.11377 tr-p2: 241.73507\n",
      "  15 tr-l1: 344.74387 tr-p1: 686.22772 tr-l2: 344.82510 tr-p2: 2.67057\n",
      "  18 tr-l1: 572.14386 tr-p1: 555.45629 tr-l2: 572.31677 tr-p2: 588.27004\n",
      "  21 tr-l1: 701.48090 tr-p1: 1377.07489 tr-l2: 701.63477 tr-p2: 25.37833\n",
      "  24 tr-l1: 858.04553 tr-p1: 528.45613 tr-l2: 858.24780 tr-p2: 1187.10771\n",
      "  27 tr-l1: 2303.62695 tr-p1: 1152.70074 tr-l2: 2303.90698 tr-p2: 3454.03105\n",
      "  30 tr-l1: 89.39185 tr-p1: 21.81206 tr-l2: 89.48676 tr-p2: 156.33270\n",
      "  33 tr-l1: 55.38745 tr-p1: 105.75906 tr-l2: 55.43496 tr-p2: 4.32655\n",
      "  36 tr-l1: 590.42163 tr-p1: 1177.32659 tr-l2: 590.50964 tr-p2: 2.93936\n",
      "  39 tr-l1: 220.19054 tr-p1: 12.58503 tr-l2: 220.24400 tr-p2: 427.35753\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 17\n",
      "ood  loss 10.01574 acc: 0.139\n",
      "e11 loss 1.33434 acc: 0.891\n",
      "e22 loss 2.49187 acc: 0.789\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 357.74368 tr-p1: 173.49218 tr-l2: 357.92569 tr-p2: 541.44217\n",
      "   3 tr-l1: 101.60004 tr-p1: 41.25705 tr-l2: 101.67848 tr-p2: 161.38715\n",
      "   6 tr-l1: 772.58295 tr-p1: 44.46228 tr-l2: 772.73920 tr-p2: 1500.11908\n",
      "   9 tr-l1: 478.01596 tr-p1: 79.21375 tr-l2: 478.14587 tr-p2: 876.17785\n",
      "  12 tr-l1: 134.57613 tr-p1: 131.57686 tr-l2: 134.67343 tr-p2: 136.90629\n",
      "  15 tr-l1: 297.84491 tr-p1: 502.61985 tr-l2: 297.90695 tr-p2: 92.45185\n",
      "  18 tr-l1: 721.63293 tr-p1: 358.95512 tr-l2: 721.83844 tr-p2: 1083.60937\n",
      "  21 tr-l1: 626.71454 tr-p1: 1037.07220 tr-l2: 626.86407 tr-p2: 215.82481\n",
      "  24 tr-l1: 371.05588 tr-p1: 419.49344 tr-l2: 371.13721 tr-p2: 322.03440\n",
      "  27 tr-l1: 604.58630 tr-p1: 732.74155 tr-l2: 604.76752 tr-p2: 475.91194\n",
      "  30 tr-l1: 481.96326 tr-p1: 566.91519 tr-l2: 482.08154 tr-p2: 396.40213\n",
      "  33 tr-l1: 788.30383 tr-p1: 597.56190 tr-l2: 788.46173 tr-p2: 978.43884\n",
      "  36 tr-l1: 607.13147 tr-p1: 1123.29228 tr-l2: 607.20966 tr-p2: 90.42274\n",
      "  39 tr-l1: 3476.45557 tr-p1: 6674.39625 tr-l2: 3476.44629 tr-p2: 277.44938\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 18\n",
      "ood  loss 10.13493 acc: 0.135\n",
      "e11 loss 1.27667 acc: 0.891\n",
      "e22 loss 2.49593 acc: 0.789\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 100.28519 tr-p1: 182.97919 tr-l2: 100.32730 tr-p2: 17.01300\n",
      "   3 tr-l1: 1229.44153 tr-p1: 80.16401 tr-l2: 1229.62671 tr-p2: 2378.13909\n",
      "   6 tr-l1: 240.34622 tr-p1: 123.37676 tr-l2: 240.38312 tr-p2: 356.60390\n",
      "   9 tr-l1: 2833.15674 tr-p1: 765.60592 tr-l2: 2833.44727 tr-p2: 4900.22488\n",
      "  12 tr-l1: 360.23611 tr-p1: 665.25890 tr-l2: 360.39499 tr-p2: 54.68103\n",
      "  15 tr-l1: 583.64020 tr-p1: 1160.99864 tr-l2: 583.73706 tr-p2: 5.71536\n",
      "  18 tr-l1: 478.00583 tr-p1: 940.33154 tr-l2: 478.11703 tr-p2: 15.13858\n",
      "  21 tr-l1: 391.65045 tr-p1: 600.47014 tr-l2: 391.75424 tr-p2: 182.28561\n",
      "  24 tr-l1: 642.27722 tr-p1: 47.00344 tr-l2: 642.47137 tr-p2: 1236.99885\n",
      "  27 tr-l1: 1633.18848 tr-p1: 1.75635 tr-l2: 1633.41138 tr-p2: 3263.99244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  30 tr-l1: 479.09314 tr-p1: 795.03721 tr-l2: 479.25421 tr-p2: 162.63939\n",
      "  33 tr-l1: 422.65970 tr-p1: 777.49738 tr-l2: 422.81619 tr-p2: 67.23967\n",
      "  36 tr-l1: 435.70200 tr-p1: 867.77704 tr-l2: 435.79816 tr-p2: 3.08469\n",
      "  39 tr-l1: 176.08873 tr-p1: 333.97598 tr-l2: 176.20604 tr-p2: 17.76657\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 19\n",
      "ood  loss 10.31274 acc: 0.145\n",
      "e11 loss 1.33506 acc: 0.889\n",
      "e22 loss 2.59996 acc: 0.788\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 444.80771 tr-p1: 886.28577 tr-l2: 444.85614 tr-p2: 2.81689\n",
      "   3 tr-l1: 293.00046 tr-p1: 1.37412 tr-l2: 293.09686 tr-p2: 584.07146\n",
      "   6 tr-l1: 358.71786 tr-p1: 138.68758 tr-l2: 358.85172 tr-p2: 578.21255\n",
      "   9 tr-l1: 1110.45605 tr-p1: 482.43487 tr-l2: 1110.71082 tr-p2: 1738.03847\n",
      "  12 tr-l1: 227.25642 tr-p1: 439.18039 tr-l2: 227.40192 tr-p2: 14.77835\n",
      "  15 tr-l1: 730.25861 tr-p1: 1198.88233 tr-l2: 730.32355 tr-p2: 261.04646\n",
      "  18 tr-l1: 582.69464 tr-p1: 627.13758 tr-l2: 582.88867 tr-p2: 537.73066\n",
      "  21 tr-l1: 368.14700 tr-p1: 672.82873 tr-l2: 368.24948 tr-p2: 62.94362\n",
      "  24 tr-l1: 425.43317 tr-p1: 0.27716 tr-l2: 425.54486 tr-p2: 849.95469\n",
      "  27 tr-l1: 648.83154 tr-p1: 33.44262 tr-l2: 649.00391 tr-p2: 1263.59984\n",
      "  30 tr-l1: 638.00299 tr-p1: 931.88500 tr-l2: 638.19464 tr-p2: 343.60578\n",
      "  33 tr-l1: 108.21445 tr-p1: 190.67028 tr-l2: 108.24303 tr-p2: 25.05594\n",
      "  36 tr-l1: 44.55139 tr-p1: 85.78191 tr-l2: 44.59561 tr-p2: 2.66343\n",
      "  39 tr-l1: 1175.28101 tr-p1: 1685.24403 tr-l2: 1175.58044 tr-p2: 665.13312\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 20\n",
      "ood  loss 9.85524 acc: 0.173\n",
      "e11 loss 1.30240 acc: 0.883\n",
      "e22 loss 2.43982 acc: 0.787\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 283.97748 tr-p1: 495.90557 tr-l2: 284.04849 tr-p2: 71.50393\n",
      "   3 tr-l1: 245.65431 tr-p1: 8.05142 tr-l2: 245.77528 tr-p2: 482.68186\n",
      "   6 tr-l1: 554.89832 tr-p1: 382.45779 tr-l2: 555.02130 tr-p2: 726.78481\n",
      "   9 tr-l1: 1228.23425 tr-p1: 74.67641 tr-l2: 1228.39282 tr-p2: 2381.16551\n",
      "  12 tr-l1: 639.35553 tr-p1: 213.76926 tr-l2: 639.51306 tr-p2: 1064.36908\n",
      "  15 tr-l1: 304.24191 tr-p1: 607.74363 tr-l2: 304.31662 tr-p2: 0.11783\n",
      "  18 tr-l1: 558.65619 tr-p1: 907.91276 tr-l2: 558.73285 tr-p2: 208.83342\n",
      "  21 tr-l1: 117.23076 tr-p1: 196.65093 tr-l2: 117.27456 tr-p2: 37.20079\n",
      "  24 tr-l1: 257.32184 tr-p1: 358.10852 tr-l2: 257.44202 tr-p2: 155.98650\n",
      "  27 tr-l1: 663.37036 tr-p1: 722.54473 tr-l2: 663.58844 tr-p2: 603.76949\n",
      "  30 tr-l1: 407.94168 tr-p1: 44.39935 tr-l2: 408.11090 tr-p2: 770.82505\n",
      "  33 tr-l1: 461.35730 tr-p1: 834.13497 tr-l2: 461.39243 tr-p2: 87.96751\n",
      "  36 tr-l1: 379.11099 tr-p1: 497.52300 tr-l2: 379.14270 tr-p2: 260.12803\n",
      "  39 tr-l1: 916.56635 tr-p1: 1546.86011 tr-l2: 916.81171 tr-p2: 285.94434\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 21\n",
      "ood  loss 10.43812 acc: 0.152\n",
      "e11 loss 1.30289 acc: 0.889\n",
      "e22 loss 2.48665 acc: 0.788\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 382.83890 tr-p1: 514.01793 tr-l2: 382.97351 tr-p2: 251.15327\n",
      "   3 tr-l1: 609.20929 tr-p1: 69.03232 tr-l2: 609.38342 tr-p2: 1148.90281\n",
      "   6 tr-l1: 403.67542 tr-p1: 85.97425 tr-l2: 403.78915 tr-p2: 720.71156\n",
      "   9 tr-l1: 559.36957 tr-p1: 680.09594 tr-l2: 559.57288 tr-p2: 438.15495\n",
      "  12 tr-l1: 477.91898 tr-p1: 789.61402 tr-l2: 478.01297 tr-p2: 165.60851\n",
      "  15 tr-l1: 506.99954 tr-p1: 999.88310 tr-l2: 507.11960 tr-p2: 13.55254\n",
      "  18 tr-l1: 326.55231 tr-p1: 540.45818 tr-l2: 326.62476 tr-p2: 112.09355\n",
      "  21 tr-l1: 178.72284 tr-p1: 342.17937 tr-l2: 178.79485 tr-p2: 14.66702\n",
      "  24 tr-l1: 366.71622 tr-p1: 78.23094 tr-l2: 366.86975 tr-p2: 654.65542\n",
      "  27 tr-l1: 409.75290 tr-p1: 29.50798 tr-l2: 409.86038 tr-p2: 789.40643\n",
      "  30 tr-l1: 272.40186 tr-p1: 433.90957 tr-l2: 272.48431 tr-p2: 110.33184\n",
      "  33 tr-l1: 558.70636 tr-p1: 967.44318 tr-l2: 558.76691 tr-p2: 149.39860\n",
      "  36 tr-l1: 588.79742 tr-p1: 1006.05534 tr-l2: 588.88824 tr-p2: 171.01811\n",
      "  39 tr-l1: 249.81110 tr-p1: 496.94288 tr-l2: 249.79462 tr-p2: 2.04618\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 22\n",
      "ood  loss 9.96090 acc: 0.174\n",
      "e11 loss 1.29492 acc: 0.884\n",
      "e22 loss 2.48786 acc: 0.788\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 128.48296 tr-p1: 211.73069 tr-l2: 128.58086 tr-p2: 44.71755\n",
      "   3 tr-l1: 708.03510 tr-p1: 26.52206 tr-l2: 708.18378 tr-p2: 1389.06222\n",
      "   6 tr-l1: 468.10046 tr-p1: 5.73435 tr-l2: 468.28769 tr-p2: 929.92773\n",
      "   9 tr-l1: 321.17520 tr-p1: 192.48190 tr-l2: 321.38452 tr-p2: 449.36556\n",
      "  12 tr-l1: 420.24197 tr-p1: 438.67542 tr-l2: 420.40585 tr-p2: 401.19663\n",
      "  15 tr-l1: 238.63039 tr-p1: 454.61152 tr-l2: 238.70766 tr-p2: 22.06109\n",
      "  18 tr-l1: 72.32732 tr-p1: 131.94220 tr-l2: 72.40183 tr-p2: 12.08212\n",
      "  21 tr-l1: 668.73236 tr-p1: 1334.49808 tr-l2: 668.81934 tr-p2: 2.52559\n",
      "  24 tr-l1: 2470.02930 tr-p1: 281.98930 tr-l2: 2470.31860 tr-p2: 4657.57772\n",
      "  27 tr-l1: 258.39044 tr-p1: 440.02500 tr-l2: 258.54211 tr-p2: 76.29714\n",
      "  30 tr-l1: 593.88513 tr-p1: 990.62324 tr-l2: 594.02563 tr-p2: 196.58222\n",
      "  33 tr-l1: 364.86575 tr-p1: 483.28163 tr-l2: 364.93604 tr-p2: 245.85652\n",
      "  36 tr-l1: 221.13843 tr-p1: 411.07815 tr-l2: 221.26486 tr-p2: 30.65170\n",
      "  39 tr-l1: 339.44424 tr-p1: 645.18065 tr-l2: 339.45584 tr-p2: 33.13787\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 23\n",
      "ood  loss 9.94176 acc: 0.161\n",
      "e11 loss 1.31375 acc: 0.886\n",
      "e22 loss 2.60235 acc: 0.789\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 215.28473 tr-p1: 338.19699 tr-l2: 215.34471 tr-p2: 91.80355\n",
      "   3 tr-l1: 545.49176 tr-p1: 31.38836 tr-l2: 545.63275 tr-p2: 1059.02310\n",
      "   6 tr-l1: 218.88503 tr-p1: 0.01784 tr-l2: 218.95352 tr-p2: 437.16631\n",
      "   9 tr-l1: 560.86493 tr-p1: 21.61147 tr-l2: 561.02563 tr-p2: 1099.56302\n",
      "  12 tr-l1: 426.57556 tr-p1: 841.30699 tr-l2: 426.61987 tr-p2: 11.24900\n",
      "  15 tr-l1: 411.33817 tr-p1: 821.89851 tr-l2: 411.48740 tr-p2: 0.28341\n",
      "  18 tr-l1: 622.07306 tr-p1: 1231.17762 tr-l2: 622.20264 tr-p2: 12.47891\n",
      "  21 tr-l1: 416.92865 tr-p1: 825.68927 tr-l2: 417.03979 tr-p2: 7.62060\n",
      "  24 tr-l1: 34.13617 tr-p1: 5.94053 tr-l2: 34.19615 tr-p2: 61.72122\n",
      "  27 tr-l1: 361.26633 tr-p1: 554.50806 tr-l2: 361.43420 tr-p2: 167.56415\n",
      "  30 tr-l1: 98.72424 tr-p1: 190.23627 tr-l2: 98.75473 tr-p2: 6.53739\n",
      "  33 tr-l1: 613.53400 tr-p1: 1225.17971 tr-l2: 613.61902 tr-p2: 1.34468\n",
      "  36 tr-l1: 240.89487 tr-p1: 451.77173 tr-l2: 240.89998 tr-p2: 29.41852\n",
      "  39 tr-l1: 340.86774 tr-p1: 576.41249 tr-l2: 340.79324 tr-p2: 104.58066\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 24\n",
      "ood  loss 9.79659 acc: 0.178\n",
      "e11 loss 1.38006 acc: 0.882\n",
      "e22 loss 2.62751 acc: 0.787\n",
      "--------------------------------------------------------------------------------\n",
      "   0 tr-l1: 288.58981 tr-p1: 370.50829 tr-l2: 288.73538 tr-p2: 206.16797\n",
      "   3 tr-l1: 475.95148 tr-p1: 145.85768 tr-l2: 476.10687 tr-p2: 805.56469\n",
      "   6 tr-l1: 52.21724 tr-p1: 102.36976 tr-l2: 52.27419 tr-p2: 1.45463\n",
      "   9 tr-l1: 1790.21838 tr-p1: 429.77207 tr-l2: 1790.48181 tr-p2: 3150.19563\n",
      "  12 tr-l1: 621.39795 tr-p1: 133.94909 tr-l2: 621.55560 tr-p2: 1108.31279\n",
      "  15 tr-l1: 673.01825 tr-p1: 1209.41689 tr-l2: 673.09906 tr-p2: 136.10697\n",
      "  18 tr-l1: 739.70709 tr-p1: 1090.51671 tr-l2: 739.93030 tr-p2: 388.38410\n",
      "  21 tr-l1: 876.41864 tr-p1: 1228.75124 tr-l2: 876.64948 tr-p2: 523.61013\n",
      "  24 tr-l1: 455.27914 tr-p1: 433.85392 tr-l2: 455.42126 tr-p2: 476.13769\n",
      "  27 tr-l1: 502.28036 tr-p1: 662.40788 tr-l2: 502.51340 tr-p2: 341.64551\n",
      "  30 tr-l1: 329.99463 tr-p1: 643.11191 tr-l2: 330.04550 tr-p2: 16.19737\n",
      "  33 tr-l1: 513.94690 tr-p1: 817.44287 tr-l2: 513.98175 tr-p2: 209.89350\n",
      "  36 tr-l1: 226.85361 tr-p1: 448.65366 tr-l2: 227.01472 tr-p2: 4.58876\n",
      "  39 tr-l1: 1301.07336 tr-p1: 2017.49727 tr-l2: 1301.11719 tr-p2: 584.21632\n",
      "--------------------------------------------------------------------------------\n",
      "epoch: 25\n"
     ]
    }
   ],
   "source": [
    "irm = IRMModel(model = get_model(n_final_units=128), \n",
    "               optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3))\n",
    "def lambda_scheduler(epoch):\n",
    "    if epoch < 2:\n",
    "        return 0\n",
    "    elif epoch < 5:\n",
    "        return 10000\n",
    "    else:\n",
    "        return 100000\n",
    "irm.train(30, lambda_scheduler, batch_size=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
